\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.1}{\ignorespaces Hippocampus and seahorse\nobreakspace {}\cite {Seress10H}\relax }}{3}{figure.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.2}{\ignorespaces Activity pattern of color encoded place cell across a maze. Each place cell is exactly related to one distinct position of the corresponding environment e.\,g.,\xspace turquoise to the first arch. Its activity spikes if the rat walks along the arch\nobreakspace {}\cite {Stuartlayton13}.\relax }}{4}{figure.caption.3}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.3}{\ignorespaces Sketched path of a rat moving in a square, while tracking firing grid cells. As their name suggests, they form a regular lattice over the space. Hence, they act as coordinate system. The information provided by grid cell is combined with that of the place cells to generate a full picture of the surroundings\nobreakspace {}\cite {Moser15PGM}.\relax }}{5}{figure.caption.5}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.4}{\ignorespaces Exemplary cognitive room{} of vehicles according to their weight and engine power. An unknown car can be placed easily in the environment given the two parameters because there are already established place cells acting as abstract waypoints (the depicted cars) to support the orientation i.\,e.,\xspace finding its place on the map. By doing so, it is immediately possible to derive information about the appearance of the automobile\nobreakspace {}\cite {BellmundEtAl18NC}.\relax }}{6}{figure.caption.6}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.5}{\ignorespaces Left: cognitive room{} of vehicles according to weight and horse power. Middle: Firing pattern of place cells crafting the cognitive room{} i.\,e.,\xspace the boundaries in Figure\nobreakspace {}\ref {fig: vehicles cognitive room}. Right: Corresponding lattice of grid cells.\nobreakspace {}\cite {BellmundEtAl18NC}\relax }}{6}{figure.caption.7}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.6}{\ignorespaces Schematic plot of the rows of state $ s^1 $ and $ s^5 $ respectively. By interpreting the ordinate values as probabilities for transitioning instead of a probability for the current state, it is possible to make assumptions on the future path the agent may take. Hence, matrix $ M $ describes all possible paths. In both cases the policy prefers pausing over changing the state.\relax }}{9}{figure.caption.10}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.7}{\ignorespaces Schematic plot of the $ s^5 $-column depicting how $ s^5 $ is reached by ascending probabilities. It is possible to recapitulate the policy consisting of pausing or taking one step to the right. Entering $ s^5 $ is most likely from $ s^4 $ and $ s^5 $ (due to resting). \relax }}{10}{figure.caption.12}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.1}{\ignorespaces The first two rules depicted as graph. In gray are corresponding $ 1 $-hot-encoded vectors for the exemplary cognitive room \texttt {[blue, to run, desk]} denoted. The rules serve as edges and the word classes as vertices.\relax }}{14}{figure.caption.14}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.2}{\ignorespaces The word to word models can also be illustrated as graphs, though more complex. The lemmatized words of the text serve as vertices and the pairs mentioned before in Section\nobreakspace {}\ref {sec: data preparation}\nobreakspace {}\nameref {sec: data preparation} as edges. In green are word vectors and in blue $ 1 $-hot-encoded vectors denoted. This cropped graph is generated by the bold passages of the text: ``\textbf {Alice sends Bob} a message. \textbf {Alice goes} to the grocery store. Peter \textbf {sent him} a letter. \textbf {Bob went} to his friend.''\relax }}{17}{figure.caption.15}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.1}{\ignorespaces Clearly visible are the successor positions of the states e.\,g.,\xspace \texttt {Adjective → Noun}. \texttt {Nouns} are smeary because the rule set in Section\nobreakspace {}\ref {enum: rule set}\nobreakspace {}\nameref {enum: rule set} doesn't provide one specifying starting on \texttt {Nouns}, so the Neural Network has to guess which results in less definite successors. Although there is a prediction made for each single word i.\,e.,\xspace state in the cognitive room{}, only word classes are displayed to avoid clutter (Pers. Pr. = \texttt {Personal Pronoun}, Pos. Pr. = \texttt {Possessive Pronoun}).\relax }}{20}{figure.caption.17}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.2}{\ignorespaces Calculating the Successor Representation for one higher time step i.\,e.,\xspace $ t=2 $, already demonstrates the properties of the construction. For all states (visual aggregated into the word class) it is possible to derive successor states e.\,g.,\xspace the two step rule \texttt {Question → Pers. Pr. → Verb}.\relax }}{21}{figure.caption.18}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.3}{\ignorespaces The illustrated plots stem from a model using additional rules backed by more word classes and a larger database to retrieve the training data. The outcomes are similar to Figure\nobreakspace {}\ref {fig: first model tpm and mds}. The upcoming states are obvious. The model can handle ambiguous successors e.\,g.,\xspace \texttt {Personal Pronoun → Verb} and \texttt {Personal Pronoun → Adverb} are deployed rules.\relax }}{22}{figure.caption.19}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.4}{\ignorespaces For illustrative purposes only a tiny data set i.\,e.,\xspace few words of the book were processed to calculate the ground truth distribution.\relax }}{23}{figure.caption.20}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.5}{\ignorespaces Transition probability matrix of model using rules constructed by processing a book und $ 1 $-hot-encoded vectors to learn. Hence, the environment is no longer manufactured. Whereas the results of the models in Section\nobreakspace {}\ref {sec: first model and architecture MR}\nobreakspace {}\nameref {sec: first model and architecture MR} were fully described by the matrix plot this will no longer be possible for large scale models because the matrix becomes to huge. As seen before can the \gls {mds} occupy this role. If sufficient learning happened, clusters or similarities to the corresponding plot of the ground truth should be recognizable. (Though not when illustrating with the tiny data set.)\relax }}{24}{figure.caption.21}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.6}{\ignorespaces Comparison of the averaged predictions (green) with the ground truth distribution (blue). The model was trained by $ 1 $-hot-encoded vectors and german data. If a \gls {pos}-tag has no blue bar, it has a $ 0 $\% probability to appear as successor e.\,g.,\xspace after an \texttt {ADJ} comes no \texttt {ADP}. The remaining plots of the other \gls {pos}-tags can be found in Appendix\nobreakspace {}\ref {ch: appendix average approach}\nobreakspace {}\nameref {ch: appendix average approach}.\relax }}{27}{figure.caption.24}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.7}{\ignorespaces Mean and standard deviation of the configurations w.\,r.\,t\onedot to the ground truth i.\,e.,\xspace the difference between the ground truth matrix and the prediction matrix was used for the row wise calculations (matrices depicted in Figure\nobreakspace {}\ref {fig: avg model gt w2v de en}). Clearly visible that $ 1 $-hot-encoded vector models (OHE) outperform word vector versions (WV). Both, mean and standard deviation are lower. Because a difference is assessed, smaller means are better.\relax }}{29}{figure.caption.26}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.8}{\ignorespaces If word vectors are used for training the model has problems with both languages. When using German the predictions degenerate completely and will classify everything as \texttt {NOUN}. The outcome of precessing English is reversed: it is quite close to an identical distribution which equals mere guessing amidst the \gls {pos}-tags.\relax }}{30}{figure.caption.28}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.1}{\ignorespaces Although mentioned that transition probability matrices don't show anything if too many words are used, they can be used sometimes to detect failure. The reference, at least for the \gls {mds}, is illustrated in Figure\nobreakspace {}\ref {fig: w2w model gt en}. The value of the metric is $ 0.50 $, the equivalent with one layer achieves $ 0.10 $. This could imply that more layers hamper learning.\relax }}{34}{figure.caption.29}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.2}{\ignorespaces As mentioned before, more layers result in worse \glspl {sr}. Already one additional hidden layer lowers the metric. The model in (a) reaches $ 0.22 $, whereas with one hidden layer the value is $ 0.10 $. The network in (b) was configured with $ 10 $ hidden layers and the successor representation looks indistinguishable from one training with $ 40 $ (Figure\nobreakspace {}\ref {fig: text model en ohe 40L}).\relax }}{35}{figure.caption.30}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.3}{\ignorespaces The only conclusion to draw from this plot of two different models is that additional epochs might lead to a full degeneration of the results if word vectors are used for training. The $ 1 $-hot-encoded vector analogue shows no mismatch to them of Section\nobreakspace {}\ref {subsubsec: multiple hidden layers}\nobreakspace {}\nameref {subsubsec: multiple hidden layers}.\relax }}{36}{figure.caption.31}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.4}{\ignorespaces \gls {mds} plot of a model using german training data and word vectors as input to learn a $ 1 $-hot-encoded vector. The results lack characteristics to draw sensible conclusions from. Though it is possible to calculate the metric for the configuration: $ 0.47 $. By comparing with Table\nobreakspace {}\ref {tab: text model versions and metrics}, it sits between its full $ 1 $-hot-encoded vector and word vector relatives.\relax }}{37}{figure.caption.32}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.5}{\ignorespaces Concatenating the training data $ 5 $ times with itself doesn't change outcomes (compare Figure\nobreakspace {}\ref {fig: text model cumulativ mds plots}). This impression is fortified by the metrics both models achieve: $ 0.14 $ for $ 1 $-hot-encoded vectors and $ 0.72 $ with word vectors ($ 0.08 $ and $ 0.74 $ without respectively, Table\nobreakspace {}\ref {tab: text model versions and metrics}).\relax }}{38}{figure.caption.33}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.6}{\ignorespaces High times also don't bring progress since no evident structure is recognizable within the plots. A comparison with the ground truth wouldn't bring additional insights too because the underlying matrices can't be interpreted as transition probability matrices.\relax }}{39}{figure.caption.34}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.7}{\ignorespaces As before in Figure\nobreakspace {}\ref {fig: high time steps ohe} no structure is recognizable to do further research on. Results relying on word vectors again seem to be very labile and degenerate.\relax }}{40}{figure.caption.35}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {A.8}{\ignorespaces The same model with german data set and word vectors as in Section\nobreakspace {}\ref {sec: text based models and architecture}\nobreakspace {}\nameref {sec: text based models and architecture} was trained. Predictions were limited to the $ 40 $ most frequent words. For a better overview the second plot was labeled with the corresponding \gls {pos}-tags.\relax }}{41}{figure.caption.36}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {B.1}{\ignorespaces \gls {mds} of ground truth using german and english training data.}}{43}{figure.caption.37}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {B.2}{\ignorespaces The $ 1 $-hot-encoded vector models show some resemblance with the ground truth in Figure\nobreakspace {}\ref {fig: text model gt de en mds}. The disappointing results of word vectors is not just visible by the different shape the dots occupy but also by their number, much less are visible i.\,e.,\xspace many are mapped onto each other. Hence the Network produces the same output for different inputs.\relax }}{44}{figure.caption.38}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.1}{\ignorespaces Barplot of \texttt {ADP} and \texttt {ADJ}.}}{45}{figure.caption.39}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.2}{\ignorespaces Barplot of \texttt {DET} and \texttt {ADV}.}}{46}{figure.caption.40}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.3}{\ignorespaces Barplot of \texttt {AUX} and \texttt {NOUN}.}}{46}{figure.caption.41}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.4}{\ignorespaces Barplot of \texttt {VERB} and \texttt {REST}.}}{47}{figure.caption.42}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {C.5}{\ignorespaces Barplot of \texttt {PART} and \texttt {PRON}.}}{47}{figure.caption.43}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
