\begin{figure}\relax \begingroup \let \protect \centering  
\includegraphics{Ortszelle_Beispiel.png}
\caption{Activity pattern of color encoded place cell across a maze. Each place cell is exactly related to one distinct position of the corresponding environment \eg turquoise to the first arch. Its activity spikes if the rat walks along the arch~\cite{Stuartlayton13}.}
\label{fig: Rat in maze}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
\includegraphics[scale=0.25]{Gitterzelle_Beispiel.png}
\caption{Sketched path of a rat moving in a square, while tracking firing grid cells. As their name suggests they form a regular lattice over the space. Hence, they act as coordinate system. The information provided by grid cell is combined with that of the place cells to generate a full picture of the surroundings~\cite{Moser15PGM}.}
\label{fig: Grid cells}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
\includegraphics[width=0.4\textwidth]{Fahrzeuge_Cognitive_Room.jpeg}
\caption{Exemplary \cognitiveroom{} of vehicles according to their weight and engine power. An unknown car can be placed easily in the environment given the two parameters because there are already established place cells acting as abstract waypoints (the depicted cars) to support the orientation \ie finding its place on the map. By doing so, it is immediately possible to derive information about the appearance of the automobile~\cite{BellmundEtAl18NC}.}
\label{fig: vehicles cognitive room}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
\includegraphics[width=1.\textwidth]{Fahrzeuge_Ortszelle_Gitterzelle.jpeg}
\caption{Left: \cognitiveroom{} of vehicles according to weight and horse power. Middle: Firing pattern of place cells crafting the \cognitiveroom{} \ie the boundaries in \figref{\ref{fig: vehicles cognitive room}}. Right: Corresponding lattice of grid cells.~\cite{BellmundEtAl18NC}}
\label{fig: vehicles with place and grid cells}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
\includegraphics[width=0.7\linewidth]{Beispiel_SR_Zeile.png}
\caption{Schematic plot of the rows of state $ s^1 $ and $ s^5 $ respectively. By interpreting the ordinate values as probabilities for transitioning instead of a probability for the current state, it is possible to make assumptions on the future path the agent may take. Hence, matrix $ M $ describes all possible paths. In both cases the policy prefers pausing over changing the state.}
\label{fig: sr-zeile}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
\includegraphics[width=0.7\textwidth]{Beispiel_SR_Spalte.png}
\caption{Schematic plot of the $ s^5 $-column depicting how $ s^5 $ is reached by ascending probabilities. It is possible to recapitulate the policy consisting of pausing or taking one step to the right. Entering $ s^5 $ is most likely from $ s^4 $ and $ s^5 $ (due to resting). }
\label{fig: sr-spalte}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
    \includegraphics[scale=0.35]{Bilder/Graphen/first_model_graph2.png}
    \caption{The first two rules depicted as graph. In gray are corresponding \onehot{s} for the exemplary cognitive room \texttt{[blue, to run, desk]} denoted. The rules serve as edges and the word classes as vertices.}
    \label{fig: first model graph}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
        \includegraphics[width=\linewidth]{Bilder/Graphen/w2w_ohe_w2v.png}
    \caption{The word to word models can also be illustrated as graphs, though more complex. The lemmatized words of the text serve as vertices and the pairs mentioned before in \secreff{sec: data preparation} as edges. In green are word vectors and in blue \onehot{s} denoted. This cropped graph is generated by the bold passages of the text: ``\textbf{Alice sends Bob} a message. \textbf{Alice goes} to the grocery store. Peter \textbf{sent him} a letter. \textbf{Bob went} to his friend.''}
    \label{fig: text model graph ohe w2v}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
\subcaptionbox{Learned Successor Representation of a tailored \cognitiveroom{} with apparent future states.}{
\includegraphics[height=\twocolpicheight]{chapter4/chapter4.1/first_models/4Rules/plots/First Model + More Rules_100E_100BS_1L_1C/Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\hfill
\subcaptionbox{\gls{mds} plot of the \gls{sr} in (a) showing decent clusters.}{
\includegraphics[height=\twocolpicheight]{chapter4/chapter4.1/first_models/4Rules/plots/First Model + More Rules_100E_100BS_1L_1C/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\caption{Clearly visible are the successor positions of the states \eg \texttt{Adjective → Noun}. \texttt{Nouns} are smeary because the rule set in \secreff{enum: rule set} doesn't provide one specifying starting on \texttt{Nouns}, so the Neural Network has to guess which results in less definite successors. Although there is a prediction made for each single word \ie state in the \cognitiveroom{}, only word classes are displayed to avoid clutter (Pers. Pr. = \texttt{Personal Pronoun}, Pos. Pr. = \texttt{Possessive Pronoun}).}
\label{fig: first model tpm and mds}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
\subcaptionbox{Calculated \gls{sr} of a tailored \cognitiveroom{} for $ t = 2 $ and $ \gamma = 0.5 $ using the matrix of \figref{\ref{fig: first model tpm and mds}}.}{
\includegraphics[height=\twocolpicheight]{chapter4/chapter4.1/first_models/4Rules/plots/First Model + More Rules_100E_100BS_1L_1C/SR,_t=2,_DF=0.5.png}
}
\hfill
\subcaptionbox{\gls{mds} plot of the \gls{sr} in (a) with properly grouped word classes}{
\includegraphics[height=\twocolpicheight]{chapter4/chapter4.1/first_models/4Rules/plots/First Model + More Rules_100E_100BS_1L_1C/MDS_of_SR,_t=2,_DF=0.5.png}
}
\caption{Calculating the Successor Representation for one higher time step \ie $ t=2 $, already demonstrates the properties of the construction. For all states (visual aggregated into the word class) it is possible to derive successor states \eg the two step rule \texttt{Question → Pers. Pr. → Verb}.}
\label{fig: first model sr t=3, df=0.5}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
\subcaptionbox{\gls{sr} of a more advanced model.}{
\includegraphics[height=0.375\textheight]{chapter4/chapter4.1/first_models/8Rules/plots/First Model + More Rules_100E_100BS_1L_1C/Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\hfill
\subcaptionbox{\gls{mds} plot of (a).}{
\includegraphics[height=0.375\textheight]{chapter4/chapter4.1/first_models/8Rules/plots/First Model + More Rules_100E_100BS_1L_1C/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\caption{The illustrated plots stem from a model using additional rules backed by more word classes and a larger database to retrieve the training data. The outcomes are similar to \figref{\ref{fig: first model tpm and mds}}. The upcoming states are obvious. The model can handle ambiguous successors \eg \texttt{Personal Pronoun → Verb} and \texttt{Personal Pronoun → Adverb} are deployed rules.}
% Learned Successor Representation (left) and corresponding \gls{mds} plot (right) of a more advanced model consisting of eight rules backed by a larger variety of words used for training.}
\label{fig: more rules and word tpm and mds}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
\subcaptionbox{Examplary ground truth distribution of a tiny data set.}{
\includegraphics[height=\twocolpicheight]{Bilder/chapter4/BspW2W/plots/OHE_OHE_500E_100BS_1L_1C_5P_30T_J/J_5pages_30T_words.png}
}
\hfill
\subcaptionbox{Corresponding \gls{mds} plot of the ground truth in (a).}{
\includegraphics[height=\twocolpicheight]{Bilder/chapter4/BspW2W/plots/OHE_OHE_500E_100BS_1L_1C_5P_30T_J/MDS_J_5pages_30T_words.png}
}
\caption{For illustrative purposes only a tiny data set \ie few words of the book were processed to calculate the ground truth distribution.}
\label{fig: text model gt}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
\subcaptionbox{Learned \srmat{} of model using \onehot{s} during training.}{
\includegraphics[height=\twocolpicheight]{chapter4/BspW2W/plots/OHE_OHE_500E_100BS_1L_1C_5P_30T_J/Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\hfill
\subcaptionbox{\gls{mds} clustering of the Successor Representation in (a).}{
\includegraphics[height=\twocolpicheight]{chapter4/BspW2W/plots/OHE_OHE_500E_100BS_1L_1C_5P_30T_J/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\caption{Transition probability matrix of model using rules constructed by processing a book und \onehot{s} to learn. Hence, the environment is no longer manufactured. Whereas the results of the models in \secreff{sec: first model and architecture MR} were fully described by the matrix plot this will no longer be possible for large scale models because the matrix becomes to huge. As seen before can the \gls{mds} occupy this role. If sufficient learning happened, clusters or similarities to the corresponding plot of the ground truth should be recognizable. (Though not when illustrating with the tiny data set.)}
\label{fig: text model sr}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
\subcaptionbox{Averaged transitions of all \texttt{ADP}s compared to the ground truth.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/Barplots/Avg_OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/_epoch-4000//Combined_Barplot_ADP_S.png}
}
\hfill
\subcaptionbox{Averaged transitions of all \texttt{ADJ}s compared to the ground truth.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/Barplots/Avg_OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/_epoch-4000//Combined_Barplot_ADJ_F.png}
}
\caption{Comparison of the averaged predictions (green) with the ground truth distribution (blue). The model was trained by \onehot{s} and german data. If a \postag{} has no blue bar, it has a $ 0 $\% probability to appear as successor \eg after an \texttt{ADJ} comes no \texttt{ADP}. The remaining plots of the other \postag{s} can be found in \appref{ch: appendix average approach}.}
\label{fig: barplot det adp}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
\includegraphics[scale=0.325, center]{code/error_com.png}
\caption{Mean and standard deviation of the configurations \wrt to the ground truth \ie the difference between the ground truth matrix and the prediction matrix was used for the row wise calculations (matrices depicted in \figref{\ref{fig: avg model gt w2v de en}}). Clearly visible that \onehot{} models (OHE) outperform word vector versions (WV). Both, mean and standard deviation are lower. Because a difference is assessed, smaller means are better.}
\label{fig: rsme plots}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
\subcaptionbox{German, ground truth of word class transitions.}{
\includegraphics[height=\threerowpicheight]{Bilder/chapter4/average_models/ground_truths/D_200pages_1500T_tags.png}
}
\hspace*{1cm}
\subcaptionbox{English, ground truth of word class transitions.}{
\includegraphics[height=\threerowpicheight]{Bilder/chapter4/average_models/ground_truths/J_200pages_1500T_tags.png}
}
\\
\subcaptionbox{German, learned \gls{sr} using \onehot{s}.}{
\includegraphics[height=\threerowpicheight]{Bilder/chapter4/average_models/plots/Avg_OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/_epochs-4000/Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\hspace*{1cm}
\subcaptionbox{English, learned \gls{sr} using \onehot{s}.}{
\includegraphics[height=\threerowpicheight]{Bilder/chapter4/average_models/plots/Avg_OHE_OHE_4000E_100BS_1L_1C_200P_1500T_J/Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\\
\subcaptionbox{German, learned \gls{sr} using word vectors.}{
\includegraphics[height=\threerowpicheight]{Bilder/chapter4/average_models/plots/Avg_W2V_W2V_5000E_100BS_1L_1C_200P_1500T_D/_epochs-4000/Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\hspace*{1cm}
\subcaptionbox{English, learned \gls{sr} using word vectors.}{
\includegraphics[height=\threerowpicheight]{Bilder/chapter4/average_models/plots/Avg_W2V_W2V_5000E_100BS_1L_1C_200P_1500T_J/_epochs-4000/Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\caption{If word vectors are used for training the model has problems with both languages. When using German the predictions degenerate completely and will classify everything as \texttt{NOUN}. The outcome of precessing English is reversed: it is quite close to an identical distribution which equals mere guessing amidst the \postag{s}.}
\label{fig: avg model gt w2v de en}
\end{figure}
\efloatseparator
 
\begin{figure}[H]\begingroup \let 
\centering
\subcaptionbox{\gls{sr} of a model using english and \onehot{s} with $ 40 $ hidden layers.}{
\includegraphics[height=\hh]{Bilder/chapter4/additional_configurations/OHE_OHE_4000E_100BS_40L_1C_200P_1500T_J/Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\hfill
\subcaptionbox{\gls{mds} of the matrix in (a).}{
\includegraphics[height=\hh]{Bilder/chapter4/additional_configurations/OHE_OHE_4000E_100BS_40L_1C_200P_1500T_J/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\caption{Although mentioned that transition probability matrices don't show anything if too many words are used, they can be used sometimes to detect failure. The reference, at least for the \gls{mds}, is illustrated in \figref{\ref{fig: w2w model gt en}}. The value of the metric is $ 0.50 $, the equivalent with one layer achieves $ 0.10 $. This could imply that more layers hamper learning.}
\label{fig: text model en ohe 40L}
\end{figure}
\efloatseparator
 
\begin{figure}[H]\begingroup \let 
\centering
\subcaptionbox{\gls{mds} of a model using english and \onehot{s} with $ 2 $ hidden layers.}{
\includegraphics[height=\hh]{Bilder/chapter4/additional_configurations/OHE_OHE_4000E_100BS_2L_1C_200P_1500T_J/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\hfill
\subcaptionbox{\gls{sr} of a model using english and \onehot{s} with $ 10 $ hidden layers.}{
\includegraphics[height=\hh]{Bilder/chapter4/additional_configurations/OHE_OHE_7000E_100BS_10L_1C_200P_1500T_J/Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\caption{As mentioned before, more layers result in worse \glspl{sr}. Already one additional hidden layer lowers the metric. The model in (a) reaches $ 0.22 $, whereas with one hidden layer the value is $ 0.10 $. The network in (b) was configured with $ 10 $ hidden layers and the successor representation looks indistinguishable from one training with $ 40 $ (\figref{\ref{fig: text model en ohe 40L}}).}
\end{figure}
\efloatseparator
 
\begin{figure}[H]\begingroup \let 
\centering
\subcaptionbox{English, \onehot{s}, $ 25,000 $ epochs and $ 40 $ layers.}{
\includegraphics[height=\twocolpicheight]{Bilder/chapter4/additional_configurations/OHE_OHE_25000E_100BS_40L_1C_200P_1500T_J/Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\hfill
\subcaptionbox{\gls{mds} of a model using english, word vectors, $ 25,000 $ epochs and $ 40 $ layers}{
\includegraphics[height=\twocolpicheight]{Bilder/chapter4/additional_configurations/W2V_W2V_25000E_100BS_40L_1C_200P_1500T_J/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\caption{The only conclusion to draw from this plot of two different models is that additional epochs might lead to a full degeneration of the results if word vectors are used for training. The \onehot{} analogue shows no mismatch to them of \secreff{subsubsec: multiple hidden layers}.}
\end{figure}
\efloatseparator
 
\begin{figure}[H]\begingroup \let 
\centering
\subcaptionbox{English, word vector to \onehot{} as \gls{mds}.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/W2V_OHE_5000E_100BS_1L_1C_200P_1500T_D/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\hfill
\subcaptionbox{German, ground truth \gls{mds} of word to word transitions.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/W2W/ground_truths/MDS_D_200pages_1500T_words.png}
}
\caption{\gls{mds} plot of a model using german training data and word vectors as input to learn a \onehot{}. The results lack characteristics to draw sensible conclusions from. Though it is possible to calculate the metric for the configuration: $ 0.47 $. By comparing with \tabref{\ref{tab: text model versions and metrics}}, it sits between its full \onehot{} and word vector relatives.}
\end{figure}
\efloatseparator
 
\begin{figure}[H]\begingroup \let 
\centering
\subcaptionbox{German, \onehot{s} with $ 5 $ concatenations.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/OHE_OHE_5000E_100BS_1L_5C_200P_1500T_D/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\hfill
\subcaptionbox{German, word vectors with $ 5 $ concatenations.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/W2V_W2V_5000E_100BS_1L_5C_200P_1500T_D/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\caption{Concatenating the training data $ 5 $ times with itself doesn't change outcomes (compare \figref{\ref{fig: text model cumulativ mds plots}}). This impression is fortified by the metrics both models achieve: $ 0.14 $ for \onehot{s} and $ 0.72 $ with word vectors ($ 0.08 $ and $ 0.74 $ without respectively, \tabref{\ref{tab: text model versions and metrics}}).}
\end{figure}
\efloatseparator
 
\begin{figure}[H]\begingroup \let 
\centering
\subcaptionbox{\gls{mds} of a \gls{sr} with $ t = 20 $. German and \onehot{s} were used during training.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/hohes_t/OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/MDS_of_SR,_t=20,_DF=0.5.png}
}
\hfill
\subcaptionbox{\gls{mds} of a \gls{sr} with $ t = 50 $. German and \onehot{s} were used during training.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/hohes_t/OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/MDS_of_SR,_t=50,_DF=0.5.png}
}
\caption{High times also don't bring progress since no evident structure is recognizable within the plots. A comparison with the ground truth wouldn't bring additional insights too because the underlying matrices can't be interpreted as transition probability matrices.}
\label{fig: high time steps ohe}
\end{figure}
\efloatseparator
 
\begin{figure}[t]\begingroup \let 
\centering
\subcaptionbox{\gls{mds} of a \gls{sr} with $ t = 20 $. German and word vectors were used during training.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/hohes_t/W2V_W2V_5000E_100BS_1L_1C_200P_1500T_D/MDS_of_SR,_t=20,_DF=0.5.png}
}
\hfill
\subcaptionbox{\gls{mds} of a \gls{sr} with $ t = 50 $. German and word vectors were used during training.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/hohes_t/W2V_W2V_5000E_100BS_1L_1C_200P_1500T_D/MDS_of_SR,_t=50,_DF=0.5.png}
}
\caption{As before in \figref{\ref{fig: high time steps ohe}} no structure is recognizable to do further research on. Results relying on word vectors again seem to be very labile and degenerate.}
\end{figure}
\efloatseparator
 
\begin{figure}[H]\begingroup \let 
\centering
\subcaptionbox{German with word vectors. \gls{mds} of the $ 40 $ most frequent words.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/MostFrequentWords_4000E_100BS_1L_1C_200P_1500T_D/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\hfill
\subcaptionbox{Same \gls{mds} plot as in (a) but annotated with \postag{s}instead of words.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/MostFrequentWords_4000E_100BS_1L_1C_200P_1500T_D/ud_pos_tag_annotated.png}
}
\caption{The same model with german data set and word vectors as in \secreff{sec: text based models and architecture} was trained. Predictions were limited to the $ 40 $ most frequent words. For a better overview the second plot was labeled with the corresponding \postag{s}.}
\end{figure}
\efloatseparator
 
\begin{figure}[H]\begingroup \let 
\centering
\subcaptionbox{German, ground truth \gls{mds} of word to word transitions.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/W2W/ground_truths/MDS_D_200pages_1500T_words.png}
}
\hfill
\subcaptionbox{English, ground truth \gls{mds} of word to word transitions.\label{fig: w2w model gt en}}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/W2W/ground_truths/MDS_J_200pages_1500T_words.png}
}
\caption[\gls{mds} of ground truth using german and english training data.]{}
\label{fig: text model gt de en mds}
\end{figure}
\efloatseparator
 
\begin{figure}\relax \begingroup \let \protect \centering  
\subcaptionbox{German, \gls{mds} of learned \gls{sr} using \onehot{s}. Metric: $ 0.08 $.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/W2W/plots/OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/_epoch-4000/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\hfill
\subcaptionbox{English, \gls{mds} of learned \gls{sr} using \onehot{s}. Metric: $ 0.1 $.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/W2W/plots/OHE_OHE_4000E_100BS_1L_1C_200P_1500T_J/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\\
\subcaptionbox{German, \gls{mds} of learned \gls{sr} using word vectors. Metric: $ 0.74 $.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/W2W/plots/W2V_W2V_5000E_100BS_1L_1C_200P_1500T_D/_epoch-4000/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\hfill
\subcaptionbox{English, \gls{mds} of learned \gls{sr} using word vectors. Metric: $ 0.78 $.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/W2W/plots/W2V_W2V_5000E_100BS_1L_1C_200P_1500T_J/_epoch-4000/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
}
\caption{The \onehot{} models show some resemblance with the ground truth in \figref{\ref{fig: text model gt de en mds}}. The disappointing results of word vectors is not just visible by the different shape the dots occupy but also by their number, much less are visible \ie many are mapped onto each other. Hence the Network produces the same output for different inputs.}
\label{fig: text model cumulativ mds plots}
\end{figure}
\efloatseparator
 
\begin{figure}[H]\begingroup \let 
\centering
\subcaptionbox{Averaged transitions of all \texttt{ADP}s compared to the ground truth.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/Barplots/Avg_OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/_epoch-4000/Combined_Barplot_ADP_S.png}
}
\hfill
\subcaptionbox{Averaged transitions of all \texttt{ADJ}s compared to the ground truth.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/Barplots/Avg_OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/_epoch-4000/Combined_Barplot_ADJ_F.png}
} 
\caption[Barplot of \texttt{ADP} and \texttt{ADJ}.]{}
\end{figure}
\efloatseparator
 
\begin{figure}[H]\begingroup \let 
\centering
\subcaptionbox{Averaged transitions of all \texttt{DET}s compared to the ground truth.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/Barplots/Avg_OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/_epoch-4000/Combined_Barplot_DET_S.png}
}
\hfill
\subcaptionbox{Averaged transitions of all \texttt{ADV}s compared to the ground truth.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/Barplots/Avg_OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/_epoch-4000/Combined_Barplot_ADV_F.png}
}
\caption[Barplot of \texttt{DET} and \texttt{ADV}.]{}
\end{figure}
\efloatseparator
 
\begin{figure}[H]\begingroup \let 
\centering
\subcaptionbox{Averaged transitions of all \texttt{AUX}s compared to the ground truth.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/Barplots/Avg_OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/_epoch-4000/Combined_Barplot_AUX_F.png}
}
\hfill
\subcaptionbox{Averaged transitions of all \texttt{NOUN}s compared to the ground truth.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/Barplots/Avg_OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/_epoch-4000/Combined_Barplot_NOUN_F.png}
}
\caption[Barplot of \texttt{AUX} and \texttt{NOUN}.]{}
\end{figure}
\efloatseparator
 
\begin{figure}[H]\begingroup \let 
\centering
\subcaptionbox{Averaged transitions of all \texttt{VERB}s compared to the ground truth.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/Barplots/Avg_OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/_epoch-4000/Combined_Barplot_VERB_S.png}
}
\hfill
\subcaptionbox{Averaged transitions of all \texttt{REST}s compared to the ground truth.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/Barplots/Avg_OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/_epoch-4000/Combined_Barplot_REST_S.png}
}
\caption[Barplot of \texttt{VERB} and \texttt{REST}.]{}
\end{figure}
\efloatseparator
 
\begin{figure}[H]\begingroup \let 
\centering
\subcaptionbox{Averaged transitions of all \texttt{PART}s compared to the ground truth.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/Barplots/Avg_OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/_epoch-4000/Combined_Barplot_PART_S.png}
}
\hfill
\subcaptionbox{Averaged transitions of all \texttt{PRON}s compared to the ground truth.}{
\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/Barplots/Avg_OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/_epoch-4000/Combined_Barplot_PRON_F.png}
}
\caption[Barplot of \texttt{PART} and \texttt{PRON}.]{}
\end{figure}
