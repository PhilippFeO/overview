\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\abx@aux@refcontext{anyt/global//global/global}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{abbreviations}{glg-abr}{gls-abr}{glo-abr}
\providecommand\@glsxtr@savepreloctag[2]{}
\@input{introduction.aux}
\abx@aux@cite{Nobelprize06}
\abx@aux@segm{0}{0}{Nobelprize06}
\abx@aux@cite{Rosenblatt58P}
\abx@aux@segm{0}{0}{Rosenblatt58P}
\abx@aux@cite{Nobelprize14}
\abx@aux@segm{0}{0}{Nobelprize14}
\abx@aux@cite{StBoGe17HPM}
\abx@aux@segm{0}{0}{StBoGe17HPM}
\abx@aux@cite{Stöwer21MA}
\abx@aux@segm{0}{0}{Stöwer21MA}
\abx@aux@backref{1}{Nobelprize06}{0}{1}{1}
\abx@aux@backref{2}{Rosenblatt58P}{0}{1}{1}
\abx@aux@backref{3}{Nobelprize14}{0}{1}{1}
\abx@aux@backref{4}{StBoGe17HPM}{0}{1}{1}
\abx@aux@backref{5}{Stöwer21MA}{0}{2}{2}
\abx@aux@cite{ORFrHa20CCN}
\abx@aux@segm{0}{0}{ORFrHa20CCN}
\abx@aux@cite{GarzorzStark18BN}
\abx@aux@segm{0}{0}{GarzorzStark18BN}
\abx@aux@cite{Seress10H}
\abx@aux@segm{0}{0}{Seress10H}
\abx@aux@cite{Seress10H}
\abx@aux@segm{0}{0}{Seress10H}
\abx@aux@cite{Trepel17N}
\abx@aux@segm{0}{0}{Trepel17N}
\abx@aux@cite{GarzorzStark18BN}
\abx@aux@segm{0}{0}{GarzorzStark18BN}
\abx@aux@cite{Stuartlayton13}
\abx@aux@segm{0}{0}{Stuartlayton13}
\abx@aux@cite{Stuartlayton13}
\abx@aux@segm{0}{0}{Stuartlayton13}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theoretical Background}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.1}Hippocampus}{3}{section.2.1}\protected@file@percent }
\newlabel{sec: Hippocampus}{{2.1}{3}{Hippocampus}{section.2.1}{}}
\newlabel{sec: Hippocampus@cref}{{[section][1][2]2.1}{[1][3][]3}}
\abx@aux@backref{6}{ORFrHa20CCN}{0}{3}{3}
\abx@aux@backref{7}{GarzorzStark18BN}{0}{3}{3}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Hippocampus and seahorse\nobreakspace  {}\cite {Seress10H}\relax }}{3}{figure.2.1}\protected@file@percent }
\abx@aux@backref{9}{Seress10H}{0}{3}{3}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{wrapfig: Hippocampus and seahorse}{{2.1}{3}{Hippocampus and seahorse~\cite {Seress10H}\relax }{figure.2.1}{}}
\newlabel{wrapfig: Hippocampus and seahorse@cref}{{[figure][1][2]2.1}{[1][3][]3}}
\abx@aux@backref{10}{Trepel17N}{0}{3}{3}
\abx@aux@backref{11}{GarzorzStark18BN}{0}{3}{3}
\abx@aux@cite{ORFrHa20CCN}
\abx@aux@segm{0}{0}{ORFrHa20CCN}
\abx@aux@cite{BellmundEtAl18NC}
\abx@aux@segm{0}{0}{BellmundEtAl18NC}
\abx@aux@cite{Moser15PGM}
\abx@aux@segm{0}{0}{Moser15PGM}
\abx@aux@cite{Moser15PGM}
\abx@aux@segm{0}{0}{Moser15PGM}
\abx@aux@cite{StBoGe17HPM}
\abx@aux@segm{0}{0}{StBoGe17HPM}
\abx@aux@cite{BellmundEtAl18NC}
\abx@aux@segm{0}{0}{BellmundEtAl18NC}
\abx@aux@cite{BellmundEtAl18NC}
\abx@aux@segm{0}{0}{BellmundEtAl18NC}
\abx@aux@cite{BellmundEtAl18NC}
\abx@aux@segm{0}{0}{BellmundEtAl18NC}
\abx@aux@cite{BellmundEtAl18NC}
\abx@aux@segm{0}{0}{BellmundEtAl18NC}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Activity pattern of color encoded place cell across a maze. Each place cell is exactly related to one distinct position of the corresponding environment e.\,g.,\xspace  turquoise to the first arch. Its activity spikes if the rat walks along the arch\nobreakspace  {}\cite {Stuartlayton13}.\relax }}{4}{figure.caption.3}\protected@file@percent }
\abx@aux@backref{13}{Stuartlayton13}{0}{4}{4}
\newlabel{fig: Rat in maze}{{2.2}{4}{Activity pattern of color encoded place cell across a maze. Each place cell is exactly related to one distinct position of the corresponding environment \eg turquoise to the first arch. Its activity spikes if the rat walks along the arch~\cite {Stuartlayton13}.\relax }{figure.caption.3}{}}
\newlabel{fig: Rat in maze@cref}{{[figure][2][2]2.2}{[1][3][]4}}
\newlabel{par: Place cell}{{2.1}{4}{Place cells}{section*.2}{}}
\newlabel{par: Place cell@cref}{{[section][1][2]2.1}{[1][3][]4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Place cells}{4}{section*.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Grid cells}{4}{section*.4}\protected@file@percent }
\abx@aux@backref{14}{ORFrHa20CCN}{0}{4}{4}
\abx@aux@backref{15}{BellmundEtAl18NC}{0}{4}{4}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.2}Predictive map theory}{4}{section.2.2}\protected@file@percent }
\newlabel{sec: predictive map theory}{{2.2}{4}{Predictive map theory}{section.2.2}{}}
\newlabel{sec: predictive map theory@cref}{{[section][2][2]2.2}{[1][4][]4}}
\abx@aux@backref{18}{StBoGe17HPM}{0}{4}{4}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Sketched path of a rat moving in a square, while tracking firing grid cells. As their name suggests, they form a regular lattice over the space. Hence, they act as coordinate system. The information provided by grid cell is combined with that of the place cells to generate a full picture of the surroundings\nobreakspace  {}\cite {Moser15PGM}.\relax }}{5}{figure.caption.5}\protected@file@percent }
\abx@aux@backref{17}{Moser15PGM}{0}{5}{5}
\newlabel{fig: Grid cells}{{2.3}{5}{Sketched path of a rat moving in a square, while tracking firing grid cells. As their name suggests, they form a regular lattice over the space. Hence, they act as coordinate system. The information provided by grid cell is combined with that of the place cells to generate a full picture of the surroundings~\cite {Moser15PGM}.\relax }{figure.caption.5}{}}
\newlabel{fig: Grid cells@cref}{{[figure][3][2]2.3}{[1][4][]5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.3}Successor Representation}{5}{section.2.3}\protected@file@percent }
\newlabel{sec: SR}{{2.3}{5}{Successor Representation}{section.2.3}{}}
\newlabel{sec: SR@cref}{{[section][3][2]2.3}{[1][5][]5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Exemplary cognitive room{} of vehicles according to their weight and engine power. An unknown car can be placed easily in the environment given the two parameters because there are already established place cells acting as abstract waypoints (the depicted cars) to support the orientation i.\,e.,\xspace  finding its place on the map. By doing so, it is immediately possible to derive information about the appearance of the automobile\nobreakspace  {}\cite {BellmundEtAl18NC}.\relax }}{6}{figure.caption.6}\protected@file@percent }
\abx@aux@backref{20}{BellmundEtAl18NC}{0}{6}{6}
\newlabel{fig: vehicles cognitive room}{{2.4}{6}{Exemplary \cognitiveroom {} of vehicles according to their weight and engine power. An unknown car can be placed easily in the environment given the two parameters because there are already established place cells acting as abstract waypoints (the depicted cars) to support the orientation \ie finding its place on the map. By doing so, it is immediately possible to derive information about the appearance of the automobile~\cite {BellmundEtAl18NC}.\relax }{figure.caption.6}{}}
\newlabel{fig: vehicles cognitive room@cref}{{[figure][4][2]2.4}{[1][4][]6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Left: cognitive room{} of vehicles according to weight and horse power. Middle: Firing pattern of place cells crafting the cognitive room{} i.\,e.,\xspace  the boundaries in Figure\nobreakspace  {}\ref  {fig: vehicles cognitive room}. Right: Corresponding lattice of grid cells.\nobreakspace  {}\cite {BellmundEtAl18NC}\relax }}{6}{figure.caption.7}\protected@file@percent }
\abx@aux@backref{22}{BellmundEtAl18NC}{0}{6}{6}
\newlabel{fig: vehicles with place and grid cells}{{2.5}{6}{Left: \cognitiveroom {} of vehicles according to weight and horse power. Middle: Firing pattern of place cells crafting the \cognitiveroom {} \ie the boundaries in \figref {\ref {fig: vehicles cognitive room}}. Right: Corresponding lattice of grid cells.~\cite {BellmundEtAl18NC}\relax }{figure.caption.7}{}}
\newlabel{fig: vehicles with place and grid cells@cref}{{[figure][5][2]2.5}{[1][4][]6}}
\abx@aux@cite{StBoGe17HPM}
\abx@aux@segm{0}{0}{StBoGe17HPM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Mathematical Foundation}{7}{subsection.2.3.1}\protected@file@percent }
\newlabel{eq: rl}{{2.1}{7}{Mathematical Foundation}{equation.2.3.1}{}}
\newlabel{eq: rl@cref}{{[equation][1][2]2.1}{[1][7][]7}}
\newlabel{eq: v with sr-matrix}{{2.2}{7}{Mathematical Foundation}{equation.2.3.2}{}}
\newlabel{eq: v with sr-matrix@cref}{{[equation][2][2]2.2}{[1][7][]7}}
\abx@aux@backref{23}{StBoGe17HPM}{0}{7}{7}
\newlabel{eq: sr or m via T}{{2.3}{7}{Mathematical Foundation}{equation.2.3.3}{}}
\newlabel{eq: sr or m via T@cref}{{[equation][3][2]2.3}{[1][7][]7}}
\newlabel{eq: geo series}{{2.4}{7}{Mathematical Foundation}{equation.2.3.4}{}}
\newlabel{eq: geo series@cref}{{[equation][4][2]2.4}{[1][7][]7}}
\abx@aux@cite{StBoGe17HPM}
\abx@aux@segm{0}{0}{StBoGe17HPM}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\gls {sr} and grid cells}{8}{section*.8}\protected@file@percent }
\abx@aux@backref{24}{StBoGe17HPM}{0}{8}{8}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Example for the Successor Representation}{8}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Rows}{8}{section*.9}\protected@file@percent }
\abx@aux@cite{HaTiFr17ESL}
\abx@aux@segm{0}{0}{HaTiFr17ESL}
\abx@aux@cite{Riess20PA}
\abx@aux@segm{0}{0}{Riess20PA}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Schematic plot of the rows of state $ s^1 $ and $ s^5 $ respectively. By interpreting the ordinate values as probabilities for transitioning instead of a probability for the current state, it is possible to make assumptions on the future path the agent may take. Hence, matrix $ M $ describes all possible paths. In both cases the policy prefers pausing over changing the state.\relax }}{9}{figure.caption.10}\protected@file@percent }
\newlabel{fig: sr-zeile}{{2.6}{9}{Schematic plot of the rows of state $ s^1 $ and $ s^5 $ respectively. By interpreting the ordinate values as probabilities for transitioning instead of a probability for the current state, it is possible to make assumptions on the future path the agent may take. Hence, matrix $ M $ describes all possible paths. In both cases the policy prefers pausing over changing the state.\relax }{figure.caption.10}{}}
\newlabel{fig: sr-zeile@cref}{{[figure][6][2]2.6}{[1][8][]9}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Columns}{9}{section*.11}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.4}Multidimensional Scaling}{9}{section.2.4}\protected@file@percent }
\abx@aux@backref{25}{HaTiFr17ESL}{0}{9}{9}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Schematic plot of the $ s^5 $-column depicting how $ s^5 $ is reached by ascending probabilities. It is possible to recapitulate the policy consisting of pausing or taking one step to the right. Entering $ s^5 $ is most likely from $ s^4 $ and $ s^5 $ (due to resting). \relax }}{10}{figure.caption.12}\protected@file@percent }
\newlabel{fig: sr-spalte}{{2.7}{10}{Schematic plot of the $ s^5 $-column depicting how $ s^5 $ is reached by ascending probabilities. It is possible to recapitulate the policy consisting of pausing or taking one step to the right. Entering $ s^5 $ is most likely from $ s^4 $ and $ s^5 $ (due to resting). \relax }{figure.caption.12}{}}
\newlabel{fig: sr-spalte@cref}{{[figure][7][2]2.7}{[1][9][]10}}
\abx@aux@backref{26}{Riess20PA}{0}{10}{10}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.5}Metric for quantifying the results}{10}{section.2.5}\protected@file@percent }
\newlabel{sec: metric}{{2.5}{10}{Metric for quantifying the results}{section.2.5}{}}
\newlabel{sec: metric@cref}{{[section][5][2]2.5}{[1][10][]10}}
\abx@aux@cite{VanRossumEtAl09Python}
\abx@aux@segm{0}{0}{VanRossumEtAl09Python}
\abx@aux@cite{chollet2015keras}
\abx@aux@segm{0}{0}{chollet2015keras}
\abx@aux@cite{harris2020array}
\abx@aux@segm{0}{0}{harris2020array}
\abx@aux@cite{Stöwer21MA}
\abx@aux@segm{0}{0}{Stöwer21MA}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {3}Framework}{13}{chapter.3}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch: framework}{{3}{13}{Framework}{chapter.3}{}}
\newlabel{ch: framework@cref}{{[chapter][3][]3}{[1][13][]13}}
\abx@aux@backref{27}{VanRossumEtAl09Python}{0}{13}{13}
\abx@aux@backref{28}{chollet2015keras}{0}{13}{13}
\abx@aux@backref{29}{harris2020array}{0}{13}{13}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.1}First Model and Architecture}{13}{section.3.1}\protected@file@percent }
\newlabel{subsec: first model and architecture}{{3.1}{13}{First Model and Architecture}{section.3.1}{}}
\newlabel{subsec: first model and architecture@cref}{{[section][1][3]3.1}{[1][13][]13}}
\abx@aux@backref{30}{Stöwer21MA}{0}{13}{13}
\newlabel{enum: rule set}{{3.1}{13}{First Model and Architecture}{section*.13}{}}
\newlabel{enum: rule set@cref}{{[section][1][3]3.1}{[1][13][]13}}
\abx@aux@cite{Glattauer06GGW}
\abx@aux@segm{0}{0}{Glattauer06GGW}
\abx@aux@cite{Gaarder96SW}
\abx@aux@segm{0}{0}{Gaarder96SW}
\abx@aux@cite{pymupdf}
\abx@aux@segm{0}{0}{pymupdf}
\abx@aux@cite{spacy2}
\abx@aux@segm{0}{0}{spacy2}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The first two rules depicted as graph. In gray are corresponding $ 1 $-hot-encoded vectors for the exemplary cognitive room \texttt  {[blue, to run, desk]} denoted. The rules serve as edges and the word classes as vertices.\relax }}{14}{figure.caption.14}\protected@file@percent }
\newlabel{fig: first model graph}{{3.1}{14}{The first two rules depicted as graph. In gray are corresponding \onehot {s} for the exemplary cognitive room \texttt {[blue, to run, desk]} denoted. The rules serve as edges and the word classes as vertices.\relax }{figure.caption.14}{}}
\newlabel{fig: first model graph@cref}{{[figure][1][3]3.1}{[1][14][]14}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.2}Word to word models}{14}{section.3.2}\protected@file@percent }
\newlabel{sec: w2w models}{{3.2}{14}{Word to word models}{section.3.2}{}}
\newlabel{sec: w2w models@cref}{{[section][2][3]3.2}{[1][14][]14}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Data preparation}{14}{subsection.3.2.1}\protected@file@percent }
\newlabel{sec: data preparation}{{3.2.1}{14}{Data preparation}{subsection.3.2.1}{}}
\newlabel{sec: data preparation@cref}{{[subsection][1][3,2]3.2.1}{[1][14][]14}}
\abx@aux@backref{31}{Glattauer06GGW}{0}{14}{14}
\abx@aux@backref{32}{Gaarder96SW}{0}{14}{14}
\abx@aux@cite{udpostags}
\abx@aux@segm{0}{0}{udpostags}
\abx@aux@cite{MikolovEtAl13DRW}
\abx@aux@segm{0}{0}{MikolovEtAl13DRW}
\abx@aux@cite{MikolovEtAl13EEW}
\abx@aux@segm{0}{0}{MikolovEtAl13EEW}
\abx@aux@cite{Wulf16ION}
\abx@aux@segm{0}{0}{Wulf16ION}
\abx@aux@backref{33}{pymupdf}{0}{15}{15}
\abx@aux@backref{34}{spacy2}{0}{15}{15}
\newlabel{item: pos tag}{{3.2.1}{15}{Data preparation}{subsection.3.2.1}{}}
\newlabel{item: pos tag@cref}{{[subsection][1][3,2]3.2.1}{[1][15][]15}}
\abx@aux@backref{36}{MikolovEtAl13DRW}{0}{15}{15}
\abx@aux@backref{37}{MikolovEtAl13EEW}{0}{15}{15}
\abx@aux@backref{35}{udpostags}{0}{15}{15}
\abx@aux@backref{38}{Wulf16ION}{0}{15}{15}
\abx@aux@cite{udpostags}
\abx@aux@segm{0}{0}{udpostags}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}$ 1 $-hot-encoded vector approach}{16}{subsection.3.2.2}\protected@file@percent }
\newlabel{subsubsec: onehot approach}{{3.2.2}{16}{\onehot {} approach}{subsection.3.2.2}{}}
\newlabel{subsubsec: onehot approach@cref}{{[subsection][2][3,2]3.2.2}{[1][15][]16}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Word vector approach}{16}{subsection.3.2.3}\protected@file@percent }
\newlabel{subsubsec: word vector approach}{{3.2.3}{16}{Word vector approach}{subsection.3.2.3}{}}
\newlabel{subsubsec: word vector approach@cref}{{[subsection][3][3,2]3.2.3}{[1][16][]16}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.3}Average approach}{16}{section.3.3}\protected@file@percent }
\newlabel{subsubsec: average approach}{{3.3}{16}{Average approach}{section.3.3}{}}
\newlabel{subsubsec: average approach@cref}{{[section][3][3]3.3}{[1][16][]16}}
\abx@aux@backref{39}{udpostags}{0}{16}{16}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The word to word models can also be illustrated as graphs, though more complex. The lemmatized words of the text serve as vertices and the pairs mentioned before in Section\nobreakspace  {}\ref  {sec: data preparation}\nobreakspace  {}\nameref  {sec: data preparation} as edges. In green are word vectors and in blue $ 1 $-hot-encoded vectors denoted. This cropped graph is generated by the bold passages of the text: ``\textbf  {Alice sends Bob} a message. \textbf  {Alice goes} to the grocery store. Peter \textbf  {sent him} a letter. \textbf  {Bob went} to his friend.''\relax }}{17}{figure.caption.15}\protected@file@percent }
\newlabel{fig: text model graph ohe w2v}{{3.2}{17}{The word to word models can also be illustrated as graphs, though more complex. The lemmatized words of the text serve as vertices and the pairs mentioned before in \secreff {sec: data preparation} as edges. In green are word vectors and in blue \onehot {s} denoted. This cropped graph is generated by the bold passages of the text: ``\textbf {Alice sends Bob} a message. \textbf {Alice goes} to the grocery store. Peter \textbf {sent him} a letter. \textbf {Bob went} to his friend.''\relax }{figure.caption.15}{}}
\newlabel{fig: text model graph ohe w2v@cref}{{[figure][2][3]3.2}{[1][16][]17}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Listing of \gls {pos}-tags.}}{18}{table.caption.16}\protected@file@percent }
\newlabel{tab: ud pos tags}{{3.1}{18}{Listing of \postag {s}}{table.caption.16}{}}
\newlabel{tab: ud pos tags@cref}{{[table][1][3]3.1}{[1][16][]18}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {4}Methodology and Results}{19}{chapter.4}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch: results}{{4}{19}{Methodology and Results}{chapter.4}{}}
\newlabel{ch: results@cref}{{[chapter][4][]4}{[1][19][]19}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.1}First Model and Architecture}{19}{section.4.1}\protected@file@percent }
\newlabel{sec: first model and architecture MR}{{4.1}{19}{First Model and Architecture}{section.4.1}{}}
\newlabel{sec: first model and architecture MR@cref}{{[section][1][4]4.1}{[1][19][]19}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Clearly visible are the successor positions of the states e.\,g.,\xspace  \texttt  {Adjective → Noun}. \texttt  {Nouns} are smeary because the rule set in Section\nobreakspace  {}\ref  {enum: rule set}\nobreakspace  {}\nameref  {enum: rule set} doesn't provide one specifying starting on \texttt  {Nouns}, so the Neural Network has to guess which results in less definite successors. Although there is a prediction made for each single word i.\,e.,\xspace  state in the cognitive room{}, only word classes are displayed to avoid clutter (Pers. Pr. = \texttt  {Personal Pronoun}, Pos. Pr. = \texttt  {Possessive Pronoun}).\relax }}{20}{figure.caption.17}\protected@file@percent }
\newlabel{fig: first model tpm and mds}{{4.1}{20}{Clearly visible are the successor positions of the states \eg \texttt {Adjective → Noun}. \texttt {Nouns} are smeary because the rule set in \secreff {enum: rule set} doesn't provide one specifying starting on \texttt {Nouns}, so the Neural Network has to guess which results in less definite successors. Although there is a prediction made for each single word \ie state in the \cognitiveroom {}, only word classes are displayed to avoid clutter (Pers. Pr. = \texttt {Personal Pronoun}, Pos. Pr. = \texttt {Possessive Pronoun}).\relax }{figure.caption.17}{}}
\newlabel{fig: first model tpm and mds@cref}{{[figure][1][4]4.1}{[1][19][]20}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Calculating the Successor Representation for one higher time step i.\,e.,\xspace  $ t=2 $, already demonstrates the properties of the construction. For all states (visual aggregated into the word class) it is possible to derive successor states e.\,g.,\xspace  the two step rule \texttt  {Question → Pers. Pr. → Verb}.\relax }}{21}{figure.caption.18}\protected@file@percent }
\newlabel{fig: first model sr t=3, df=0.5}{{4.2}{21}{Calculating the Successor Representation for one higher time step \ie $ t=2 $, already demonstrates the properties of the construction. For all states (visual aggregated into the word class) it is possible to derive successor states \eg the two step rule \texttt {Question → Pers. Pr. → Verb}.\relax }{figure.caption.18}{}}
\newlabel{fig: first model sr t=3, df=0.5@cref}{{[figure][2][4]4.2}{[1][19][]21}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The illustrated plots stem from a model using additional rules backed by more word classes and a larger database to retrieve the training data. The outcomes are similar to Figure\nobreakspace  {}\ref  {fig: first model tpm and mds}. The upcoming states are obvious. The model can handle ambiguous successors e.\,g.,\xspace  \texttt  {Personal Pronoun → Verb} and \texttt  {Personal Pronoun → Adverb} are deployed rules.\relax }}{22}{figure.caption.19}\protected@file@percent }
\newlabel{fig: more rules and word tpm and mds}{{4.3}{22}{The illustrated plots stem from a model using additional rules backed by more word classes and a larger database to retrieve the training data. The outcomes are similar to \figref {\ref {fig: first model tpm and mds}}. The upcoming states are obvious. The model can handle ambiguous successors \eg \texttt {Personal Pronoun → Verb} and \texttt {Personal Pronoun → Adverb} are deployed rules.\relax }{figure.caption.19}{}}
\newlabel{fig: more rules and word tpm and mds@cref}{{[figure][3][4]4.3}{[1][22][]22}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.2}Word to word models}{22}{section.4.2}\protected@file@percent }
\newlabel{sec: text based models and architecture}{{4.2}{22}{Word to word models}{section.4.2}{}}
\newlabel{sec: text based models and architecture@cref}{{[section][2][4]4.2}{[1][22][]22}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces For illustrative purposes only a tiny data set i.\,e.,\xspace  few words of the book were processed to calculate the ground truth distribution.\relax }}{23}{figure.caption.20}\protected@file@percent }
\newlabel{fig: text model gt}{{4.4}{23}{For illustrative purposes only a tiny data set \ie few words of the book were processed to calculate the ground truth distribution.\relax }{figure.caption.20}{}}
\newlabel{fig: text model gt@cref}{{[figure][4][4]4.4}{[1][23][]23}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Transition probability matrix of model using rules constructed by processing a book und $ 1 $-hot-encoded vectors to learn. Hence, the environment is no longer manufactured. Whereas the results of the models in Section\nobreakspace  {}\ref  {sec: first model and architecture MR}\nobreakspace  {}\nameref  {sec: first model and architecture MR} were fully described by the matrix plot this will no longer be possible for large scale models because the matrix becomes to huge. As seen before can the \gls {mds} occupy this role. If sufficient learning happened, clusters or similarities to the corresponding plot of the ground truth should be recognizable. (Though not when illustrating with the tiny data set.)\relax }}{24}{figure.caption.21}\protected@file@percent }
\newlabel{fig: text model sr}{{4.5}{24}{Transition probability matrix of model using rules constructed by processing a book und \onehot {s} to learn. Hence, the environment is no longer manufactured. Whereas the results of the models in \secreff {sec: first model and architecture MR} were fully described by the matrix plot this will no longer be possible for large scale models because the matrix becomes to huge. As seen before can the \gls {mds} occupy this role. If sufficient learning happened, clusters or similarities to the corresponding plot of the ground truth should be recognizable. (Though not when illustrating with the tiny data set.)\relax }{figure.caption.21}{}}
\newlabel{fig: text model sr@cref}{{[figure][5][4]4.5}{[1][23][]24}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Trained models with metric values regarding their corresponding ground truth. ``german'' and ``english'' refer to the book which provided the data set (s. Section\nobreakspace  {}\ref  {sec: data preparation}\nobreakspace  {}\nameref  {sec: data preparation}). The associated \gls {mds} plots for a qualitative feedback can be found in Figure\nobreakspace  {}\ref  {fig: text model gt de en mds} and Figure\nobreakspace  {}\ref  {fig: text model cumulativ mds plots} of Appendix\nobreakspace  {}\ref  {ch: appendix text model}\nobreakspace  {}\nameref  {ch: appendix text model}.\relax }}{25}{table.caption.22}\protected@file@percent }
\newlabel{tab: text model versions and metrics}{{4.1}{25}{Trained models with metric values regarding their corresponding ground truth. ``german'' and ``english'' refer to the book which provided the data set (s. \secreff {sec: data preparation}). The associated \gls {mds} plots for a qualitative feedback can be found in \figref {\ref {fig: text model gt de en mds}} and \figref {\ref {fig: text model cumulativ mds plots}} of \appref {ch: appendix text model}.\relax }{table.caption.22}{}}
\newlabel{tab: text model versions and metrics@cref}{{[table][1][4]4.1}{[1][23][]25}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Word to word models: Evaluating the results}{25}{subsection.4.2.1}\protected@file@percent }
\newlabel{subsec: text model evaluating the results}{{4.2.1}{25}{Word to word models: Evaluating the results}{subsection.4.2.1}{}}
\newlabel{subsec: text model evaluating the results@cref}{{[subsection][1][4,2]4.2.1}{[1][23][]25}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Additional configurations}{25}{section*.23}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Schematic sentences for \gls {pos}-tag rules}}{26}{table.caption.25}\protected@file@percent }
\newlabel{tab: example sentences}{{4.2}{26}{Schematic sentences for \postag {} rules}{table.caption.25}{}}
\newlabel{tab: example sentences@cref}{{[table][2][4]4.2}{[1][26][]26}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.3}Averaging models}{26}{section.4.3}\protected@file@percent }
\newlabel{sec: average approach}{{4.3}{26}{Averaging models}{section.4.3}{}}
\newlabel{sec: average approach@cref}{{[section][3][4]4.3}{[1][26][]26}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Averaging models: Evaluating the results}{26}{subsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Comparison of the averaged predictions (green) with the ground truth distribution (blue). The model was trained by $ 1 $-hot-encoded vectors and german data. If a \gls {pos}-tag has no blue bar, it has a $ 0 $\% probability to appear as successor e.\,g.,\xspace  after an \texttt  {ADJ} comes no \texttt  {ADP}. The remaining plots of the other \gls {pos}-tags can be found in Appendix\nobreakspace  {}\ref  {ch: appendix average approach}\nobreakspace  {}\nameref  {ch: appendix average approach}.\relax }}{27}{figure.caption.24}\protected@file@percent }
\newlabel{fig: barplot det adp}{{4.6}{27}{Comparison of the averaged predictions (green) with the ground truth distribution (blue). The model was trained by \onehot {s} and german data. If a \postag {} has no blue bar, it has a $ 0 $\% probability to appear as successor \eg after an \texttt {ADJ} comes no \texttt {ADP}. The remaining plots of the other \postag {s} can be found in \appref {ch: appendix average approach}.\relax }{figure.caption.24}{}}
\newlabel{fig: barplot det adp@cref}{{[figure][6][4]4.6}{[1][26][]27}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Averaged means of the difference between prediction and ground truth. As expected, the word vector versions have higher scores than the $ 1 $-hot-encoded vector counterpart. These values can't be used for a comparison with Table\nobreakspace  {}\ref  {tab: text model versions and metrics} because the underlying metrics are different.\relax }}{28}{table.caption.27}\protected@file@percent }
\newlabel{tab: avg model versions and metrics}{{4.3}{28}{Averaged means of the difference between prediction and ground truth. As expected, the word vector versions have higher scores than the \onehot {} counterpart. These values can't be used for a comparison with \tabref {\ref {tab: text model versions and metrics}} because the underlying metrics are different.\relax }{table.caption.27}{}}
\newlabel{tab: avg model versions and metrics@cref}{{[table][3][4]4.3}{[1][28][]28}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Mean and standard deviation of the configurations w.\,r.\,t\onedot  to the ground truth i.\,e.,\xspace  the difference between the ground truth matrix and the prediction matrix was used for the row wise calculations (matrices depicted in Figure\nobreakspace  {}\ref  {fig: avg model gt w2v de en}). Clearly visible that $ 1 $-hot-encoded vector models (OHE) outperform word vector versions (WV). Both, mean and standard deviation are lower. Because a difference is assessed, smaller means are better.\relax }}{29}{figure.caption.26}\protected@file@percent }
\newlabel{fig: rsme plots}{{4.7}{29}{Mean and standard deviation of the configurations \wrt to the ground truth \ie the difference between the ground truth matrix and the prediction matrix was used for the row wise calculations (matrices depicted in \figref {\ref {fig: avg model gt w2v de en}}). Clearly visible that \onehot {} models (OHE) outperform word vector versions (WV). Both, mean and standard deviation are lower. Because a difference is assessed, smaller means are better.\relax }{figure.caption.26}{}}
\newlabel{fig: rsme plots@cref}{{[figure][7][4]4.7}{[1][26][]29}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces If word vectors are used for training the model has problems with both languages. When using German the predictions degenerate completely and will classify everything as \texttt  {NOUN}. The outcome of precessing English is reversed: it is quite close to an identical distribution which equals mere guessing amidst the \gls {pos}-tags.\relax }}{30}{figure.caption.28}\protected@file@percent }
\newlabel{fig: avg model gt w2v de en}{{4.8}{30}{If word vectors are used for training the model has problems with both languages. When using German the predictions degenerate completely and will classify everything as \texttt {NOUN}. The outcome of precessing English is reversed: it is quite close to an identical distribution which equals mere guessing amidst the \postag {s}.\relax }{figure.caption.28}{}}
\newlabel{fig: avg model gt w2v de en@cref}{{[figure][8][4]4.8}{[1][28][]30}}
\@input{conclusion.aux}
\abx@aux@cite{StBoGe17HPM}
\abx@aux@segm{0}{0}{StBoGe17HPM}
\abx@aux@backref{40}{StBoGe17HPM}{0}{31}{31}
\abx@aux@cite{StBoGe17HPM}
\abx@aux@segm{0}{0}{StBoGe17HPM}
\abx@aux@backref{41}{StBoGe17HPM}{0}{32}{32}
\@input{appendixAdditionalConfigurations.aux}
\@input{appendixA.aux}
\@input{appendixB.aux}
\@input{appendixParameters.aux}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{List of Abbreviations}{51}{section*.45}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{List of Figures}{53}{appendix*.46}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{List of Tables}{59}{appendix*.47}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{Bibliography}{61}{appendix*.48}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{0312B62B3EE4E99D994686CB05EC2A7D}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{BellmundEtAl18NC}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{chollet2015keras}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{udpostags}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Gaarder96SW}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{GarzorzStark18BN}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Glattauer06GGW}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{harris2020array}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{HaTiFr17ESL}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{spacy2}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{pymupdf}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{MikolovEtAl13DRW}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{MikolovEtAl13EEW}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Moser15PGM}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Nobelprize06}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Nobelprize14}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{ORFrHa20CCN}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Riess20PA}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Rosenblatt58P}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Seress10H}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{StBoGe17HPM}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Stöwer21MA}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Stuartlayton13}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Trepel17N}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{VanRossumEtAl09Python}{anyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Wulf16ION}{anyt/global//global/global}
\gdef \@abspage@last{71}
