%%\subsection{Additional configurations} \label{subsec: additional configurations}
%To draw a full picture, plenty of approaches which had the goal to improve the results will be mentioned in this chapter. Sadly, no one changed the outcome by any means. Facing this presented an enormous obstacle while researching. Some of them will be presented shortly in this chapter. In all cases it is obvious that these configurations were dead ends.
%
%The architectures included
%\begin{itemize}
%	\item different number of epochs from $ 100 $ to $ 50000 $
%	\item $ 2 $-hot-encoded vectors
%	\item an auto-encoder structure
%\end{itemize}
%
%% --------------------------------------
%
%\section{Multiple hidden layers} \label{subsubsec: multiple hidden layers}
%Different numbers of layers ranging from $ 1 $ to $ 100 $ were tested, some example results will be depicted. 
%\begin{figure}[H]
%	\centering
%	\subcaptionbox{\gls{sr} of a model using english and \onehot{s} with $ 40 $ hidden layers.}{
%		\includegraphics[height=\twocolpicheight]{Bilder/chapter4/additional_configurations/OHE_OHE_4000E_100BS_40L_1C_200P_1500T_J/Transition_Probability_Matrix;_t=1,_DF=0.5.png}
%	}
%	\hfill
%	\subcaptionbox{\gls{mds} of the matrix in (a).}{
%		\includegraphics[height=\twocolpicheight]{Bilder/chapter4/additional_configurations/OHE_OHE_4000E_100BS_40L_1C_200P_1500T_J/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
%	}
%	\caption{Although mentioned that transition probability matrices don't show anything if too many words are used, they can be used sometimes to detect failure. The reference, at least for the \gls{mds}, is illustrated in \figref{\ref{fig: w2w model gt en}}. The value of the metric is $ 0.50 $, the equivalent with one layer achieves $ 0.10 $ (s. \tabref{\ref{tab: text model versions and metrics}} to get a full list). This could imply that more layers hamper learning.}
%	\label{fig: text model en ohe 40L}
%\end{figure}
%%
%\begin{figure}[H]
%	\centering
%		\subcaptionbox{\gls{mds} of a model using english and \onehot{s} with $ 2 $ hidden layers.}{
%			\includegraphics[height=\twocolpicheight]{Bilder/chapter4/additional_configurations/OHE_OHE_4000E_100BS_2L_1C_200P_1500T_J/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
%		}
%		\hfill
%		\subcaptionbox{\gls{sr} of a model using english and \onehot{s} with $ 10 $ hidden layers.}{
%			\includegraphics[height=\twocolpicheight]{Bilder/chapter4/additional_configurations/OHE_OHE_7000E_100BS_10L_1C_200P_1500T_J/Transition_Probability_Matrix;_t=1,_DF=0.5.png}
%		}
%	\caption{As mentioned before, more layers result in worse \glspl{sr}. Already one additional hidden layer lowers the metric. The model in (a) reaches $ 0.22 $, whereas with one hidden layer the value is $ 0.10 $. The network in (b) was configured with $ 10 $ hidden layers and the successor representation looks indistinguishable from one training with $ 40 $ (\figref{\ref{fig: text model en ohe 40L}}).}
%\end{figure}
%
%% --------------------------------------
%
%\section{Many epochs combined with multiple hidden layers}
%The example outputs stem from a model which trained with sixfold epochs and $ 40 $ hidden layers.
%\begin{figure}[H]
%	\centering
%		\subcaptionbox{English, \onehot{s}, $ 25,000 $ epochs and $ 40 $ layers.}{
%			\includegraphics[height=\twocolpicheight]{Bilder/chapter4/additional_configurations/OHE_OHE_25000E_100BS_40L_1C_200P_1500T_J/Transition_Probability_Matrix;_t=1,_DF=0.5.png}
%		}
%		\hfill
%		\subcaptionbox{\gls{mds} of a model using english, word vectors, $ 25,000 $ epochs and $ 40 $ layers}{
%			\includegraphics[height=\twocolpicheight]{Bilder/chapter4/additional_configurations/W2V_W2V_25000E_100BS_40L_1C_200P_1500T_J/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
%		}
%	\caption{The only conclusion to draw from this plot of two different models is that additional epochs might lead to a full degeneration of the results if word vectors are used for training. The \onehot{} analogue shows no mismatch to them of \secreff{subsubsec: multiple hidden layers}.}
%\end{figure}
%
%% --------------------------------------
%
%\section{Using word vectors to learn an \onehot{}}
%This configuration is combination of two mainly used in the thesis. It uses word vectors as input and \onehot{s} as output \ie it uses heterogeneous structured training data.
%\begin{figure}[H]
%	\centering
%		\subcaptionbox{English, word vector to \onehot{} as \gls{mds}.}{
%			\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/W2V_OHE_5000E_100BS_1L_1C_200P_1500T_D/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
%		}
%	\hfill
%		\subcaptionbox{German, ground truth \gls{mds} of word to word transitions.}{
%			\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/W2W/ground_truths/MDS_D_200pages_1500T_words.png}
%		}
%	\caption{\gls{mds} plot of a model using german training data and word vectors as input to learn a \onehot{}. The results lack characteristics to draw sensible conclusions from. Though it is possible to calculate the metric for the configuration: $ 0.47 $. By comparing with \tabref{\ref{tab: text model versions and metrics}}, it sits between its full \onehot{} and word vector relatives.}
%\end{figure}
%
%% --------------------------------------
%
%\section{Multiplying the training data} \label{subsubsec: multiplying training data}
%The goal of multiplying the training data \ie concatenating the training data $ n $ times with itself, was to have the opportunity to see the training data more often during one epoch.
%\begin{figure}[H]
%	\centering
%		\subcaptionbox{German, \onehot{s} with $ 5 $ concatenations.}{
%			\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/OHE_OHE_5000E_100BS_1L_5C_200P_1500T_D/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
%		}
%		\hfill
%		\subcaptionbox{German, word vectors with $ 5 $ concatenations.}{
%			\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/W2V_W2V_5000E_100BS_1L_5C_200P_1500T_D/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
%		}
%	\caption{Concatenating the training data $ 5 $ times with itself doesn't change outcomes (compare \figref{\ref{fig: text model cumulativ mds plots}}). This impression is fortified by the metrics both models achieve: $ 0.14 $ for \onehot{s} and $ 0.72 $ with word vectors ($ 0.08 $ and $ 0.74 $ without respectively, \tabref{\ref{tab: text model versions and metrics}}).}
%\end{figure}
%
%% --------------------------------------
%
%\section{Calculating high time steps}
%One idea was to calculate high time steps of the \gls{sr} hoping the irregularities even out in distant future.
%\begin{figure}[H]
%	\centering
%		\subcaptionbox{\gls{mds} of a \gls{sr} with $ t = 20 $. German and \onehot{s} were used during training.}{
%			\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/hohes_t/OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/MDS_of_SR,_t=20,_DF=0.5.png}
%		}
%		\hfill
%		\subcaptionbox{\gls{mds} of a \gls{sr} with $ t = 50 $. German and \onehot{s} were used during training.}{
%			\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/hohes_t/OHE_OHE_5000E_100BS_1L_1C_200P_1500T_D/MDS_of_SR,_t=50,_DF=0.5.png}
%		}
%	\caption{High times also don't bring progress since no evident structure is recognizable within the plots. A comparison with the ground truth wouldn't bring additional insights too because the underlying matrices can't be interpreted as transition probability matrices.}
%	\label{fig: high time steps ohe}
%\end{figure}
%%
%\begin{figure}[H]
%	\centering
%		\subcaptionbox{\gls{mds} of a \gls{sr} with $ t = 20 $. German and word vectors were used during training.}{
%			\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/hohes_t/W2V_W2V_5000E_100BS_1L_1C_200P_1500T_D/MDS_of_SR,_t=20,_DF=0.5.png}
%		}
%		\hfill
%		\subcaptionbox{\gls{mds} of a \gls{sr} with $ t = 50 $. German and word vectors were used during training.}{
%			\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/hohes_t/W2V_W2V_5000E_100BS_1L_1C_200P_1500T_D/MDS_of_SR,_t=50,_DF=0.5.png}
%		}
%	\caption{As before in \figref{\ref{fig: high time steps ohe}} no structure is recognizable to do further research on. Results relying on word vectors again seem to be very labile and degenerate.}
%\end{figure}
%
%% --------------------------------------
%
%\section{Predict only the most frequent words}
%Similar to \secreff{subsubsec: multiplying training data}, the most frequent words of the text are seen more often by the network. Hence it might be able to learn these inputs better than ordinary ones.
%\begin{figure}[H]
%	\centering
%		\subcaptionbox{German with word vectors. \gls{mds} of the $ 40 $ most frequent words.}{
%			\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/MostFrequentWords_4000E_100BS_1L_1C_200P_1500T_D/MDS_of_Transition_Probability_Matrix;_t=1,_DF=0.5.png}
%		}
%		\hfill
%		\subcaptionbox{Same \gls{mds} plot as in (a) but annotated with \postag{s}instead of words.}{
%			\includegraphics[width=\twocolpicwidth]{Bilder/chapter4/additional_configurations/MostFrequentWords_4000E_100BS_1L_1C_200P_1500T_D/ud_pos_tag_annotated.png}
%		}
%	\caption{The same model with german data set and word vectors as in \secreff{sec: text based models and architecture} was trained. Predictions were limited to the $ 40 $ most frequent words. For a better overview the second plot was labeled with the corresponding \postag{s}.}
%\end{figure}