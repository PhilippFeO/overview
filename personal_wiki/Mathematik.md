# Mathematik
Angelegt Donnerstag 31 MÃ¤rz 2022

[./Differential und Linienintegral.pdf](./Mathematik_files/Mathematik/Differential und Linienintegral.pdf) 
[./Linienintegral und Skalarfeld.pdf](./Mathematik_files/Mathematik/Linienintegral und Skalarfeld.pdf)

* [Kurvenintegral 1. & 2. Art â€“ YouTube > Weitz](https://www.youtube.com/watch?v=7mrsZzXmibg)
* [Gebietsintegral â€“ YouTube > Weitz](https://www.youtube.com/watch?v=u3qYaKv0Ffo)
* [Playlist Differentialgeometrie â€“ YouTube > Weitz](https://www.youtube.com/watch?v=dFrSAXwDtlk&list=PLb0zKSynM2PD3i3xMuWrUF9_txMrJMGEZ)

[./FlÃ¤chenelement.pdf](./Mathematik_files/Mathematik/FlÃ¤chenelement.pdf) 
[./Integralrechnung Papula.pdf](./Mathematik_files/Mathematik/Integralrechnung Papula.pdf) 


* [Playlist MaÃŸtheorie â€“ YouTube > The Bright Side of Mathematics](https://www.youtube.com/watch?v=4DHP8cBcg_o&list=PLBh2i93oe2qskb2hCIR2HfO4ZFF1LXO1d)


Diverses
--------

* f:D â†’ R, x â†’ f(x), dann
	* f(x **+** 1): Verschieben des **Ursprungs nach rechts **um 1, bzw. des **Graphen nach links** um 1
	* f(x **-** 1): Analog nur nach links (Ursprung) und rechts (Graph)
	* f(x) **+** a: Verschieben des **Ursprungs nach unten** um 1, bzw. des **Graphen nach oben** um a
	* f(x) **-** a: Analog nur nach oben (Ursprung) und unten (Graph)

In der Praxis, bspw. Differentialquotient (Variable ist h **nicht** x!):

* f(x~0~ + h) - f(x~0~):
	* f wird um x~0~ nach links und um f(x~0~) nach unten verschoben. Das bedeutet im Prinzip, dass der Punkt (x~0~, f(x~0~)) nun der neue Ursprung ist
	* Das Koordinatensystem wird um x~0~ nach rechts und f(x~0~) nach oben verschoben.


* **Potenzreihen** sind diff'bar, wenn sie âˆ€ xâˆˆ(-r, r) konvergiert, dh. âˆ‘~nâˆˆN~ a~n~x^n^ ist endlich. Sie wird dann gliedweise (per Summenregel) differenziert, bspw. kann man so die e-Funktion ableiten


Integration
-----------

### Substitutionsregel und Koordinatentransformation

#### Koordinatentransformation
[Integration durch Substitution (Substitutionsregel) und Transformationssatz â€“ Weitz (YouTube)](https://www.youtube.com/watch?v=geJ-36mnZ1I)
![](./Mathematik_files/Mathematik/pasted_image.png)

* Der Ausdruck Ï†'(x) dient als â€Korrekturfaktorâ€œ (s. Bild unten)
* Auf der rechten Seite muss man sich y als etwas vorstellen, dass von einer Variablen, hier x, abhÃ¤ngt.
* Man kann die Gleichung auch anderweitig formulieren:

âˆ«~a~^b^ f(y) dy = âˆ«~Ï†â»Â¹(a)~^Ï†â»Â¹(b) ^f(Ï†(x)) Â· Ï†'(x) dx
(In diesem Fall muss Ï†^-1^ existieren. Das ist bei der Formulierung auf der Aufnahme nicht notwendig.)

* Die Umformulierung entspricht dann einer â€Koordinatentransformationâ€œ und wird bspw. bei Wegintegralen, dh. Ï† ist ein Weg Î³, verwendet.

![](./Mathematik_files/Mathematik/pasted_image002.png)

* WÃ¼rde man die Integrale ohne den Korrekturfaktor Ï†'(x) notieren, wÃ¤re das entsprechende Integral zu klein. Das kann man an dem Beispiel der Bildschirmaufnahme gut erkennen.

Hier wÃ¤re Ï†(x) = 2x (und Ï†^-1^(y) = 1/2y).
Um von â€obenâ€œ nach â€untenâ€œ zu gelangen, werden die Rechtecke der Riemann-Summen um den Faktor 1/2 gestaucht (bzw. um von â€untenâ€œ nach â€obenâ€œ zu gelangen mit 2 gestreckt). Das bedeutet, dass das ursprÃ¼ngliche Rechteck mit der FlÃ¤che f(2x) * **2** * dx nun die FlÃ¤che f(2x) * dx hat, also kleiner ist. Um dieses Missstand zu beheben, ist ein Korrekturfaktor nÃ¶tig und der lautet in diesem Fall 2, bzw. Ï†'(x). Durch die Multiplikation mit 2 bzw. Ï†'(x) wird die Funktion einfach doppelt so hoch.
WÃ¤re Ï†(x) anders, wÃ¤re der FlÃ¤chenzuwachs nicht so schÃ¶n linear

* Eine andere Herangehensweise: Ausgangspunkt ist âˆ«~aÂ²~^bÂ²^ f(y) dy, wobei nun y durch xÂ² ersetzt wird, dh. man hat nun âˆ«~a~^b ^f(xÂ²) dx. Allerdings fehlt noch der Korrekturfaktor bei den Rechtecken (s. Bild unten).

![](./Mathematik_files/Mathematik/pasted_image003.png)
Unten haben wir die HÃ¶he f(xÂ²) und die Breite dx, ergo f(xÂ²) dx. Oben haben wir die HÃ¶he f(y) = f(xÂ²) aber **nicht** die Breite dy. Wegen Ï†(x) = xÂ² verÃ¤ndert sich diese in AbhÃ¤ngigkeit von x. (Im Video wurde an dieser Stelle eine Animation abgespielt, in der man sah, dass die unteren Rechtecke immer gleichbreit waren, wÃ¤hrend sich die oben in ihrer Breite kontinuierlich verÃ¤nderten.)
Wir wÃ¼ssten nun gerne in welchem VerhÃ¤ltnis die Breite oben zu der unten steht, dh. wie stehen dy/dx zu einander?
Durch die suggestive Leibnizschreibweise ist das nun leicht herauszufinden:
dy/dx = dxÂ²/dx = 2x â‡” dy = dxÂ² = 2x dx 
Damit haben wir nun dy und fÃ¼r die obere RechtecksflÃ¤che ergibt sich: f(y) dy = f(xÂ²) 2x dx.

#### Transformationssatz
Dieser ist mit der obigen AusfÃ¼hrung hoffentlich leicht zu verstehen:
âˆ«~A~ f(Î¦(v)) Â· |det(DÎ¦(x))| dx = âˆ«~B~ f(w) dw
wobei v, w Vektoren und D der mehrdimensionale Ableitungsoperator ist, dh. DÎ¦ ist die Jacobi-Matrix des Diffeomorphismus' Î¦. Als Bild:
![](./Mathematik_files/Mathematik/pasted_image001.png)
Eine typische Anwendung fÃ¼r den Transformationsatz ist eine Integration Ã¼ber Kreis-fÃ¶rmigen Gebieten, indem man zu Polarkoordinaten Ã¼bergeht. Wird auch in obigem Video (<https://www.youtube.com/watch?v=geJ-36mnZ1I>) erlÃ¤utert.

* Man kann den Transformationssatz auch maÃŸtheoretisch formulieren. s. dazu [MaÃŸtheorie#**MaÃŸtheoretischer Transformationssatz**](#Mathematik:MaÃŸtheorie).
* [Transformationssatz âˆ’ Wikipedia](https://de.wikipedia.org/wiki/Transformationssatz)


### Wegintegrale und Transformationssatz
Das Wegintegral ist fÃ¼r f: â„^n^ â†’ â„ und Î³: [a, b] â†’ â„^n^ wie folgt definiert:
âˆ«~Î³([a,b])~ f ds := âˆ«~a~^b^ f(Î³(t)) Â· ||Î³'(t)||~2~ dt
Das ist im Prinzip **fast** der *Transformationssatz* fÃ¼r Dimension 1 bzw. *Integration durch Substitution* âˆ’ nur passt || Â· ||~2~ nicht so recht zu beidem. **Das liegt daran**, dass Î³ kein [Diffeomorphismus âˆ’ Wikipedia](https://de.wikipedia.org/wiki/Diffeomorphismus) ist (Î³ bildet bspw. nicht in sich selbst ab).
Letztenendes passt aber doch alles unter einen Hut, da man beim *Transformationssatz* allgemeiner von der [(verallgemeinerte) Funktionaldeterminante âˆ’ Wikipedia](https://de.wikipedia.org/wiki/Funktionaldeterminante) spricht. Man kann dann auch die Selbstabbildungseigenschaft von Î¦, dh A,B âŠ†R^m^, fallen lassen. Diese ist wie folgt fÃ¼r f: R^n^ â†’ R^m^ (n,m nicht unbedingt gleich) definiert:
Ff(x) := âˆš(det(Df(x)^T^ Â· Df(x)))
(A^T^A ist eine quadratische symmetrische Matrix fÃ¼r alle A)
(In Df(x) stehen die partiellen Ableitungen als Funktion, keine Zahlen, wie bei f'(x))
Die [(verallgemeinerte) Funktionaldeterminante âˆ’ Wikipedia](https://de.wikipedia.org/wiki/Funktionaldeterminante) wird auch [Gramsche Determinante âˆ’ Wikipedia](https://de.wikipedia.org/wiki/Gramsche_Determinante) genannt.
Mit diesem Hintergrund ergibt sich fÃ¼r das obige Wegintegral, wenn man den *Transformationssatz* mit Î³ anwendet:
âˆ«~Î³([a,b])~ f ds = âˆ«~a~^b^ f(Î³(t)) Â· FÎ³(t) dt = âˆ«~a~^b^ f(Î³(t)) Â· ||Î³'(t)||~2~ dt,
wobei FÎ³(t) = ||Î³'(t)||~2~, wie unten stehende Rechnung zeigt:
Î³'(t) = DÎ³(t) = [ x'(t) y'(t) ]^T^
â‡’ DÎ³(t)^T^ Â· DÎ³(t) = x'^2^ + y'^2^
â‡’ det(DÎ³(t)^T^ Â· DÎ³(t)) = x'^2^ + y'^2^
â‡’ âˆš(det(DÎ³(t)^T^ Â· DÎ³(t))) = âˆš(x'^2^ + y'^2^) = ||Î³'(t)||~2~

* [Wegintegral âˆ’ Wikipedia](https://de.wikipedia.org/wiki/Kurvenintegral)


#### Beispiel, bzw. Substitutionsregel

* âˆ«~a~^b^ f(x) dx = âˆ«~uâ»Â¹(a)~^uâ»Â¹(b) ^f(x(u)) * x'(u) du

Die Variable x wird als Funktion von u ausgedrÃ¼ckt.
**Ziel** ist es durch eine geschickte Wahl von x(u) den Integranden zu vereinfachen

* Beispiel: âˆ«~a~^b^ âˆš(1-xÂ²) dx

x(u) := sin(u)
u(y) := arcsin(y)
x'(u) = dx/du â‡” cos(u) = dx/du â‡” dx = cos(u) du
â‡’ âˆ«~a~^b^ âˆš(1-xÂ²) dx
= âˆ«~arcsin(a)~^arcsin(b)^ âˆš(1-x(u)Â²) * x'(u) du
= âˆ«~arcsin(a)~^arcsin(b)^ âˆš(1-sinÂ²(u)) * cos(u) du
= âˆ«~arcsin(a)~^arcsin(b)^ âˆšcosÂ²(u) * cos(u) du
= âˆ«~arcsin(a)~^arcsin(b)^ cosÂ²(u) du
![](./Mathematik_files/Mathematik/pasted_image005.png)

Faltung
-------
Gegeben seien zwei Funktionen f,g: â„¤/â„ â†’ â„¤/â„/â„‚. Die (diskrete) Faltung ist definiert als

* f*g(k) := âˆ‘~nâˆˆâ„¤~ f(n)g(âˆ’n+k) = âˆ‘~nâˆˆâ„¤~ f(n)g(kâˆ’n)
* f*g(x) := âˆ«~â„~ f(t)g(âˆ’t+x) dt = âˆ«~â„~ f(t)g(xâˆ’t) dt (Die Summen werden im Kontinuierlichen einfach Integralek)

Dabei wird g als *Faltungskern* oder *Filter* bezeichnet. Der Ausdruck âˆ’n+k=kâˆ’n, bzw. âˆ’t+x=xâˆ’t, im Argument von g bedeutet dabei:

* âˆ’n: Spiegelung [[BegrÃ¼ndung fÃ¼r Spiegelung](#Mathematik)] an der y-Achse
	* Damit die Spiegelung nicht ins Gewicht fÃ¤llt, verwendet man als Filter meist symmetrische Filter.
* +k: intuitives â€Durchschiebenâ€ des Filters von links nach rechts.

[id: begrÃ¼ndung-spiegelung][BegrÃ¼ndung fÃ¼r Spiegelung]:

* Die Spiegelung hat ihren Ursprung in der Signalverarbeitung, da sie dort automatisch auftritt.
* Im diskreten 2d-Fall mit einer *Filtermatrix* kann man es auch erlÃ¤utern (s. unten)
* â€Spiegelung und + (dh. das eigentliche Verschieben des Graphen nach links)â€œ wird zu einem Verschieben nach rechts, vor allem bei symmetrischen Faltungskernen


* Mit einer Faltung mÃ¶chte man verschiedenes erreichen:
	* Ãœbertragen der Eigenschaften des *Faltungskerns* auf die Funktion, bspw. um eine nicht Stetige glatt zu machen
	* Extraktion von Merkmalen (Feature extraction), dh. man mÃ¶chte bestimmte Elemente, bspw. ein Smiley, eines Bildes finden. Dazu nimmt man eine Matrix und beschreibt dieses Element in dieser, bspw. indem man es mit 1en und 0en Pixel-Art-mÃ¤ÃŸig nachbaut und diese Matrix dann Ã¼ber das Bild schiebt. Hohe Werte (in Relation zu den anderen EintrÃ¤gen der Ergebnismatrix; in Relation deswegen, weil ein Bild/eine Matrix, die Ã¼berall groÃŸe Werte aufweist, dann diese auch im Ergebnis hat und dann bringen objektive hohe Zahlen nichts) sprechen dann dafÃ¼r, dass dort das Element gut getroffen wurde.
		* Wenn man die *Filtermatrix* baut, muss man noch darauf achten, dass sie im Zuge der Faltung gespiegelt wird, s. [#2d-faltung-mit-filtermatrix](#Mathematik) oder man fÃ¼hrt **bewusst** eine [Korrelation](#Mathematik) durch.
		* Dieses Prinzip funktioniert auch im Kontinuierlichen 1d-Szenario aber hier fÃ¤llt mir kein anschauliches Beispiel ein.


### Gleitender Durchschnitt

* Mittels g = 1/Î¼(A) * 1~A~ (Indikator auf Menge A, durch xâˆ’t kann man A um x verschieben, s. unten) kann man per Faltung den gleitenden Durchschnitt einer Funktion f bzgl. A berechnen, dh. f*g(x) hat an jeder Stelle x den durchschnittlichen Wert, den f hat, wenn man f nur auf der Menge A um x herum betrachtet und dort den Durchschnitt (FlÃ¤che/Integral durch MaÃŸ der Menge) berechnet. Den Indikator kann man per xâˆ’t verschieben. Man kann A um x herum legen, indem man 1~A~(x-t) rechnet.

Algorithmisch ausgedrÃ¼ckt:

* Nimm f und x
* Nimm A, das fÃ¼r die Anschauung am besten symmetrisch ist, bspw. A = [xâˆ’1/3, x+1/3] oder eine Kugel B~x~(r), weil dann Î¼(A+x) = Î¼(A) gilt
* [id: indikator-umformung]Multipliziere f und g(xâˆ’t) = 1/Î¼(A) * 1~A~(xâˆ’t) = 1/Î¼(A+x) * 1~A+x~(âˆ’t) = 1/Î¼(A+x) * 1~A+x~(t) (Die Menge verschiebt man mit + nicht wie Graphen mit âˆ’; aus âˆ’t wird t wegen der Achsensymmetrie)

â‡’ f ist also nur â€umâ€œ die Stelle x ungleich 0 (Die AnfÃ¼hrungszeichen, weil â€umâ€œ nur dann Sinn macht, wenn die Menge in AbhÃ¤ngigkeit von x definiert wurde). Bspw. wird f mit A+x = [xâˆ’1/3, x+1/3] â€links und rechtsâ€œ abgeschnitten.

* Berechne den Durchschnitt von f auf dieser Menge, dh FlÃ¤che/Integral durch MaÃŸ der Menge


* Als Integral, wobei 
	* 1~[âˆ’a, a]~(xâˆ’t) â‰¡ 1~[xâˆ’a, x+a]~(t) fÃ¼r eine Variable t und ein festes x (s. [#indikator-umformung](#Mathematik))
	* Î¼(A+x) = Î¼(A) (Translationsinvarianz von MaÃŸen)

f*g(x) = âˆ«~â„~ f(t) * g(xâˆ’t) dt
  = âˆ«~â„~ f(t) * 1/Î¼(A+x) * 1~A+x~(t) dt, [#indikator-umformung](#Mathematik) & wenn A von einem Punkt abhÃ¤ngt
[id: formel]= 1/Î¼(A) * âˆ«~A+x~ f(t) dt
â‡’ Gleitende Durchschnitte sind Faltungen mit dem entsprechenden Indikator

#### Verallgemeinerung

* nimmt man ein allgemeines g aber mit endlichem TrÃ¤ger supp(g) (dh. Menge der Nicht-Nullstellen hat MaÃŸ > 1), berechnet man auch einen gleitenden Durchschnitt aber gewichtet f gemÃ¤ÃŸ g in diesem Fenster.
	* Man kann g wie folgt darstellen, um obiger [#formel](#Mathematik) nÃ¤her zu kommen: g = 1/Î¼(supp(g)) * 1~supp(g)~ * g

f * g(x) = âˆ«~â„~ f(t) * 1/Î¼(supp(g)) * 1~supp(g)~(xâˆ’t) * g(xâˆ’t) dt
  = 1/Î¼(supp(g)) * âˆ«~supp(g)+x~ f(t) * g(xâˆ’t) dt

* Man kann sich ein allgemeines g wie eine Multiplikation mit dem Indikator vorstellen, um beliebige Kurven zu erhalten
* Um die Rechnung â€schÃ¶nerâ€œ zu gestalten, nimmt man meist ein symmetrisches g mit TrÃ¤ger 1, dh. Î¼(supp(g)) = 1.


### 2d-Faltung mit Filtermatrix

* im 2d-Fall wird eine *(gespiegelte)* *Filtermatrix* von links nach rechts und oben nach unten Ã¼ber das Bild geschoben, Ã¼berlappende Punkte multipliziert und die Produkte addiert (vgl. @cnn-Schicht in einem Neuralen Netz)
* Die Matrix wird aus folgendem Grund horizontal und vertikal gespiegelt: Angenommen, man wÃ¼rde das nicht machen, ergÃ¤be sich folgende Rechnung, [id: korrelation]*Korrelation* genannt:

![](./Mathematik_files/Mathematik/Korrelation Filtermatrix Beispiel.jpg)
Hierbei wird die *Filtermatrix*, wie oben beschrieben, Ã¼ber das Bild geschoben aber **ohne** Spiegelung. Das Resultat entspricht der horizontal und vertikal gespiegelten *Filtermatrix*. Das ist jedoch unpraktisch (und auch unintuitiv), da man an sich mÃ¶chte, dass sich der Filter â€direktâ€œ im Resultat wiederfindet, vor allem wenn die *Filtermatrix* eine bestimmte Form, bspw. ein âœ˜ oder ein Smiley, beschreibt. Das kann man dadurch beheben, indem man die *Filtermatrix* erst horizontal und vertikal spiegelt und dann Ã¼ber das Bild schiebt. Diese Operation wird *Konvolution* oder *Faltung* genannt und entspricht dann einer Spiegelung mit anschlieÃŸender *Korrelation*:
![](./Mathematik_files/Mathematik/Konvolution Faltung Filtermatrix Beispiel.jpg)
â‡’ Die h. & v. Spiegelung nimmt man eher aus â€Bequemlichkeitâ€œ vor, damit das Resultat am Ende â€schÃ¶ner aussiehtâ€œ.

* In einem Neuralen Netz mit @cnn-Schicht muss man wÃ¤hrend der Implementierung nicht auf die Spiegelung achten, weil das Netz die Gewichte, dh. die *Filtermatrix-EintrÃ¤ge* lernt. Erst, wann man eine Konfiguration lÃ¤dt, muss die Spiegelung vornehmen, bevor man das Netz laufen lÃ¤sst.


Stencil-Methoden
----------------

* Mit Hilfe von Taylor-Reihen kann man Ableitungen von Funktionen in AbhÃ¤ngigkeit von dieser selbst darstellen (vgl. [Two_Point_Stencil_Example âˆ’ Wikipedia [en]](https://en.wikipedia.org/wiki/Compact_stencil#Two_Point_Stencil_Example)).
* f(x~0~âˆ“h) bedeutet, dass man f auf einem Gitter mit Schrittweite h auswertet


# 3Blue1Brown
Angelegt Sonntag 02 Oktober 2022

Videos, die 3Blue1Brown empfiehlt:

* [Summer of Math Exposition 2-Playlist â€“ YouTube](https://www.youtube.com/playlist?list=PLnQX-jgAF5pTZXPiD8ciEARRylD9brJXU)
* Weitere, besonders empfehlenswerte, unter [Have you seen more math videos in your feed recently? (SoME2 results) â€“ 3Blue1Brown (YouTube)](https://www.youtube.com/watch?v=cDofhN-RJqg)
	* â˜‘ [How to Take the Factorial of Any Number â€“ Lines That Connec (YouTube)](https://www.youtube.com/watch?v=v_HeaeUUOnc&t=0s)
* [How are memories stored in neural networks? | The Hopfield Network #SoME2  â€“ YouTube](https://www.youtube.com/watch?v=piF6D6CQxUw)
* â˜‘ [What A General Diagonal Argument Looks Like (Category Theory) â€“ YouTube](https://www.youtube.com/watch?v=dwNxVpbEVcc)
* â˜‘ [Proofs & Goofs Ep1: Linear Programming #SoME2 â€“ YouTube](https://www.youtube.com/watch?v=pLNZbykPDOA&list=PLnQX-jgAF5pTZXPiD8ciEARRylD9brJXU&index=215)
* â˜‘ [Introduction to Projective Geometry via Tic-Tac-Toe Grids #SoME2 â€“ YouTube](https://www.youtube.com/watch?v=mTw3o8-xMIo&list=PLnQX-jgAF5pTZXPiD8ciEARRylD9brJXU&index=18)
* [The Shadowy World of Umbral Calculus â€“ YouTube](https://www.youtube.com/watch?v=D0EUFP7-P1M&list=PLnQX-jgAF5pTZXPiD8ciEARRylD9brJXU&index=21)
* â˜‘ [Duality: magic in simple geometry #SoME2 â€“ YouTube](https://www.youtube.com/watch?v=SWrWlgGJe3k&list=PLnQX-jgAF5pTZXPiD8ciEARRylD9brJXU&index=36)
* [How prime numbers protect your privacy #SoME2 â€“ YouTube](https://www.youtube.com/watch?v=BScMvVH6U4E&list=PLnQX-jgAF5pTZXPiD8ciEARRylD9brJXU&index=93)

![](./Mathematik_files/Mathematik/3Blue1Brown/Mathe YouTube.png)
[./Mathe YouTube.png](./Mathematik_files/Mathematik/3Blue1Brown/Mathe YouTube.png)

# Deep Learning
Angelegt Sonntag 13 November 2022


* Teilweise Zusammenfassung [./Deep Learning Architectures: A Mathematical Approach-Springer âˆ’ Ovidiu Calin (Kommentare).pdf](./Mathematik_files/Mathematik/Deep_Learning/Deep Learning Architectures: A Mathematical Approach-Springer âˆ’ Ovidiu Calin (Kommentare).pdf)
* Weitere Informationen:
	* [Informatik:**Backpropagation**](#Informatik:Backpropagation)
	* [Informatik:**Deep Learning with Python**](#Informatik:Deep Learning with Python)
	* [Informatik:**Lexikon**](#Informatik:Lexikon)
	* [Informatik:**Reinforcement Learning**](#Informatik:Reinforcement Learning)


Allgemeines
-----------

* *Negative Likelihood function* fÃ¼r Dichte q: -l~q~(x) := -ln q(x) (negativer Logarithmus von q). Allgemeine Eigenschaften auf S46.



### Netztwerke als Funktionsapproximierer

* kapitelS. 16: Netzwerke mit wenigstens einer *versteckten Schicht* kÃ¶nnen praktisch alle Funktionen lernen/approximieren, wenn Daten aus Kompaktum stammen.
* x = Eingabe, y = Berechnete Ausgabe des Netzwerks, dh. f~w,b~(x), z = Wert der Zielfunktion an Stelle x, Î¦(x) (im Allgemeinen unbekannt und nur durch Punkte, dh. Eingabedaten und Labels gegeben)
	* y=f~w,b~(x) abhÃ¤ngig von x, w (Gewichten) und b ([#bias](#Mathematik:Deep Learning))
* So gesehen entsprechen die Trainingsdaten, dh. Eingabe x und Labels z, Punkten der unbekannten und deswegen zu lernenden Funktion Î¦, so wie (x, f(x)) = (x, y) Punkte eines 1d-Funktionsgraphen beschreiben. Die Gewichte sind dabei die Stellschrauben/Parameter (wie a,b,c in axÂ²+bx+c), um den vorgegebenen Punkten mÃ¶glichst nahe zu kommen. Hat man gute Gewichte gefunden (und fixiert), hat man die Zielfunktion gefunden und kann nun das Netzwerk als Funktion verwenden, indem man Eingabedaten, bspw. Bilder abbildet.
* FÃ¼r w*, b* := arg min~w,b~ C (die Werte von w, b an denen die [Kostenfunktion](#Mathematik:Deep Learning) C ihr Minimum annimmt)

â‡’ Dieser Prozess wird *Learning*, bzw. *Lernen* genannt

#### Zufallsvariablen

* Es kÃ¶nnen auch Zufallsvariablen Z gelernt werden, dh. Eingabe ist Zufallsvariable und Ausgabe des Netzwerks, dh Y=f~w,b~(X) sind ZVen. Das habe ich aber noch nicht verstanden TODO


2 Activation Funktions
----------------------

* FÃ¼r nicht lineare Funktionen benÃ¶tigt man nicht-lineare Aktivierungsfunktionen (AF)


### Kapitel 2.1 Examples of Activation Functions

* Definitionen einiger AF, wie Heaviside-Funktion
* 3 Arten von AF:
	* Hockey-SchlÃ¤ger-Funktionen
		* Netzwerke mit ReLU lernen deutlich schneller (bei Bildern sogar besser) als mit einer â€sÃ¤ttigendenâ€œ AF wie sigmoid(x), logistic, hyperbolic tangent
		* ReLU, PReLU(Î±, x), ELU(Î±, x) (linear fÃ¼r x>0, negativ & exponential fÃ¼r xâ‰¤0, @differenzierbar fÃ¼r Î±=1), SELU(Î±, Î», x) := Î» Â· ELU, SLU (Produkt aus linearer Funktion und sigmoid, evtl. besser als ReLU), softplus (glatte Variante von ReLU), ...
	* Sigmoid-Funktionen
		* sigmoid Ïƒ (glatt, symmetrisch, kann Sprungfunktionen beliebig genau approximieren, Sprungfunktionen werden gerne â€als Schwellenwertâ€œ verwendet, bildet â„ auf (0, 1) ab, Ïƒ~c~'(x) = cÏƒ~c~(1 âˆ’ Ïƒ~c~(x), kann auf 2-Parameter erweitert werden), Inverse von sigmoid Ïƒ: logit, tanh = 2Ïƒ~2~ âˆ’ 1 (tanh' = 1 âˆ’ tanhÂ², deswegen gut fÃ¼r Backpropagation, symmetrisch um den Ursprung ist Vorteil gegenÃ¼ber sigmoid Ïƒ), softsign, ...
	* Glockenkurven-Funktionen (â€bumped-shapedâ€œ)
		* GauÃŸ-Kurve, Doppele Exponentialfunktion (e^-Î»|x|^)
* Klassifizierungsfunktionen (31), kommen also nur in der letzten Schicht zum Tragen:
	* softmax(x) = y mit y~i~ = e^x_i^/ ||e^x^|| (||e^x^|| = âˆ‘e^x_i^), um 1-hot-encoded Vektor zu erreichen.
		* Man kann auch Parameter einbauen.
		* Glatte Version der Maximumsfunktion


### 2.4 Summary
The most common nonlinear activation functions used for neural networks are the standard logistic sigmoid and the hyperbolic tangent. Their differentiability and the fact that their derivatives can be represented in terms of the functions themselves make these activation functions useful when applying the backpropagation algorithm.

3 Cost functions
----------------

* Kostenfunktion gibt an, in welchem Sinne (LÂ², euklidischer Abstand, etc.) die Zielfunktion z=Î¦(x) approximiert werden soll. s. auch [Netztwerke als Funktionsapproximierer](#Mathematik:Deep Learning).
* Notation: C(w, b), um zu betonen, dass eigentlich die Entwicklung von C in AbhÃ¤ngigkeit von den Gewichten interessant ist, dh. man betrachtet im Allgemeinen y=f~w,b~(x) und z=Î¦(x) als konstant und probiert dann w, b so zu wÃ¤hlen, dass C(w, b) fÃ¼r mÃ¶glichst viele y=f~w,b~(x), z=Î¦(x) minimiert wird.
	* Unter'm Strich ist C sehr wohl von y, z abhÃ¤ngig und man mÃ¼sste eigentlich C(w, b, y, z) schreiben. LÃ¤sst man Letztere Weg werden sie als konstant oder â€unwichtigâ€œ angenommen (was auch immer das in dem jeweiligen Kontext heiÃŸen muss)


### 3.2 - 3.x Beispiele von Kostenfunktionen

* 3.2 Supremumsfunktion
* LÂ²-Norm: glatt in w,b
* Mean Square Error: ist relativ beliebt, GrÃ¼nde auf S44; gut, mÃ¶chte man Zufallsvariablen lernen (C(w, b) = E[ (Y âˆ’ Z)Â² ])


B: Tensoren
-----------

* *Ordnung*: Anzahl der Dimensionen (in [:Python](#Python): ``len(tensor.shape) == np.ndim(tensor)``)
* *Typ*: Dimensionstupel, bspw. d~1~Ã—d~2~Ã—d~3~, 3Ã—3, ... (in [:Python](#Python): ``tensor.shape``)
* Beispiele:
	* RGB-Bild: Tensor der Ordnung 3 vom Typ nÃ—mÃ—3

C: Measure Theory
-----------------

* s. auch [:Mathematik:**MaÃŸtheorie**](#Mathematik:MaÃŸtheorie)
* *p-System*: Teilmengensystem, das abgeschlossen unter Schnitten ist 




Bias
----

* In der Theorie wird der Bias subtrahiert, WX âˆ’ b. Durch hinzunahme einer zusÃ¤tzlichen Komponente mit x~n+1~ = âˆ’1 und analog fÃ¼r W, dh. erweitern um eine Spalte, sodass w~i,j+1~ = b~i~, erhÃ¤lt man X^*^ & W^*^ und wird die Subtraktion los: W^*^X^*^.



Fehler
------

* p. 24: "positive and linear for x > 4" â‡’ for x > 0
* p. 38: "for image-related tasks [90] and [45]." â‡’ tasks, see [90] and [45]. ("see" used everywhere else)
* p. 704: "This means that the mistakes that are negligible in the system Î½ [Î¼?], i.e., Î¼(A) = 0, also pass undetected by the system Î¼ [Î½?], i.e. Î½(A) = 0." â‡’ In Remark C.7.2 may Î½ and Î¼ be interchanged respectively.






PhÃ¤nomen A ist x-verteilt, bspw. Poisson- oder Normalverteilt. x hat ein MaÃŸ P, bspw. das Poisson-MaÃŸ (das â€NormalverteilungsmaÃŸâ€œ gibt es nicht). MÃ¶chte man jetzt irgendetwas Ã¼ber A wissen, muss man die Menge M entsprechend modellieren und in P einsetzen. FÃ¼r P(M) kann man dann den Ausdruck verwenden oder das Integral Ã¼ber die entsprechende Dichte.







# Funktionalanalysis
Angelegt Sonntag 13 November 2022


* â€Funktion f durch g im LÂ²-Sinne approximierenâ€œ bedeuet: ||f-g||~LÂ²~ = âˆ« (f-g)Â²(x) dx ist minimal


AbschÃ¤tzungen fÃ¼r Normen
------------------------

* [Cauchy-Schwarzsche Ungleichung âˆ’ Wikipedia](https://de.wikipedia.org/wiki/Cauchy-Schwarzsche_Ungleichung) fÃ¼r Vektoren x,y bzw. Vektorraum mit Skalarprodukt:

|âŸ¨x, yâŸ©|Â² â‰¤  âŸ¨x, xâŸ© Â· âŸ¨y, yâŸ©
â‡” âŸ¨x, xâŸ© Â· âŸ¨y, yâŸ© âˆ’ |âŸ¨x, yâŸ©|Â² â‰¥ 0

* < wenn x,y linear unabhÃ¤ngig sind.
* Nur LÂ² (und â„“Â²) haben ein Skalarprodukt, s. [Lp-Raum#Der Hilbertraum LÂ² âˆ’ Wikipedia](https://de.wikipedia.org/wiki/Lp-Raum#Der_Hilbertraum_L2), dh. man kann âŸ¨Â·, Â·âŸ© durch Summen und Integrale ersetzen, evtl. noch fÃ¼r die Funktionen die IdentitÃ¤t nehmen, um wirklich x & y stehen zu haben.

@abschÃ¤tzung @ungleichung @norm @summe @integral


# Lebesgue-Integral
Angelegt Sonntag 13 November 2022


* Man kann Ableitung und Integration fÃ¼r eine Lebesgue-integrierbare Funktion f vertauschen, dh

âˆ‚~x~[ âˆ«~Î©~ f(x) dx ] = âˆ«~Î©~ âˆ‚~x~[ f(x) ] dx

* [differentiation-under-the-integral-sign-for-lebesgue-integrable-derivative âˆ’ math.stackexchange](https://math.stackexchange.com/questions/94628/differentiation-under-the-integral-sign-for-lebesgue-integrable-derivative)

  


# Lineare Algebra
Angelegt Dienstag 31 Mai 2022

**Centering Matrix**/**Zentrierungsmatrix**: <https://en.wikipedia.org/wiki/Centering_matrix>

Matrix-Vektor-Produkt
---------------------
Gegeben:
``v = (1 1), M = (2 0  ==> M * v = (2 2)``
``		0 2)``
Das Produkt ``M * v`` kann auf zwei Arten interpretiert werden (vgl. [Koordinatentransformation âˆ’ Wikipedia [de]](https://de.wikipedia.org/wiki/Koordinatentransformation)):
Kurz gesagt:

1. normale Abbildung (â€Aliasâ€œ)
2. Darstellung desselben Objektes bzgl einer anderen Basis (â€Alibiâ€œ)


ad 1.) ``M`` **bildet die Basisvektoren auf neue Basisvektoren ab**. In Folge dessen wird auch ``v`` auf einen neuen Vektor linear abgebildet (so, wie eine Funktion einen Wert neu abbildet; Eine lineare Abbildung ist dadurch eindeutig bestimmt, was sie mit den Basisvektoren anstellt). In diesem Fall werden die BV ``(1 0)``, ``(0 1)`` skaliert auf ``(2 0)``, ``(0 2)`` und damit ``v`` auf ``(2 2)``.

* LÃ¤ge man das alte und das neue Koords. Ã¼bereinander, so wÃ¼rden sich ``v`` und ``Mv`` **nicht Ã¼berdecken** (hier: â€``Mv`` ragt weiter hinausâ€œ).
* Da man Matrix ``M`` in dieser Interpretation als â€normaleâ€œ Abbildung betrachtet gilt hier fÃ¼r die Basen:

``M``: Vek. in alter Basis â†’ Vek. in alter Basis
â‡’ Es findet **kein** Basiswechsel statt
ad 2.) ``M * v = (2 2)`` stellt ``v`` bzgl. der Basis ``(1/2 0)``, ``(0 1/2)`` dar, dh.
``   M = (1/2 0     = (2 0``
``        0   1\2)``^``-1``^``    0 2)``,
â‡’ ``M`` wird also als Inverse der Matrix aus den neuen Basisvektoren dargestellt. Da man die Inverse und damit das entsprechende Koords. nur schlecht ablesen kann, kann man es selten so schÃ¶n wie hier darstellen. In Formeln:
Vektor ``w`` bzgl. neuer Basis ``f``~``1``~, ``f``~``2``~ darstellen: ``(f``~``1``~`` f``~``2``~``)``^``-1``^`` * w`` (die kleine ^``-1``^ beachten!)

* LÃ¤ge man das alte und das neue Koords. Ã¼bereinander, so wÃ¼rden sich ``v`` und ``Mv`` **Ã¼berdecken**.
* Umgekehrt: MÃ¶chte man die Koordinaten eines Vektors ``w`` in einem anderen Koords. mit den Basisvektoren ``f``~``1``~, ``f``~``2``~ haben, muss man ihn mit ``(f``~``1``~`` f``~``2``~``)``^``-1``^ multiplizieren
* Allgemein gilt fÃ¼r Basiswechsel (``{e``~``1``~, ``â€¦``, ``e``~``n``~``}`` alte Basis, ``{f``~``1``~, ``â€¦``, ``f``~``n``~``}`` neue Basis):

``Mat(e``~``1``~, ``â€¦``, ``e``~``n``~``)``: Vek. in neuer Basis â†’ Vek. in alter ``e``-Basis
``Mat(f``~``1``~, ``â€¦``, ``f``~``n``~``)``^``-1``^: Vek. in alter Basis â†’ Vek. in neuer Basis (fÃ¼r die neue Basis mÃ¼ssen deren Basisvektoren invertiert werden)

* MÃ¶chte man Vektor ``w``, gegeben in (alter) Basis ``B``, bzgl der (neuen) Basis ``C`` darstellen, so muss man

``Mat(c``~``1``~``, â€¦, c``~``n``~``)``^``-1``^`` * w``
rechnen.


Eigenwerte & -vektoren
----------------------

19. [Eigenwerte und -vektoren](#Mathematik:Lineare Algebra:Eigenwerte und -vektoren)









# Eigenwerte und -vektoren
Angelegt Dienstag 31 Mai 2022


* [Wissenswertes zur Eigenwertzerlegung â€“ Wikipedia](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Useful_facts_regarding_eigenvalues)
* s. auch [Lineare Algebra](#Mathematik:Lineare Algebra), um mehr Ã¼ber Basiswechsel zu erfahren.



* Da (vollbesetzte) Matrizen umstÃ¤ndliche Objekte sind, mÃ¶chte man sie vereinfachen, um mit ihnen besser arbeiten zu kÃ¶nnen. Dabei sind (Block-)Diagonalformen das Leichteste, was man erreichen kann. Um eine Matrix ``A``, die in diesem Fall als â€normaleâ€œ lineare Abbildung interpretiert wird (vgl. oben!), zu transformieren, benÃ¶tigt man ihre *Eigenwerte* und *Eigenvektoren*.
* [Eigenschaften von Eigenwerte & -vektoren âˆ’ Wikipedia [de]](https://de.wikipedia.org/wiki/Eigenwertproblem#Eigenschaften)


### Berechnung

* ``Av = Î»v <=> (A-Î»E)v = 0 <=> det(A-Î»E) = 0``. Die LÃ¶sungen werden *Eigenwerte* genannt.
	* ``det(A-Î»E)`` beschreibt ein Polynom in ``Î»`` und ist deswegen immer lÃ¶sbar
* mit den *EW* kÃ¶nnen per ``(A-Î»E)v = 0`` die *EV* ``Î±``~``i``~ berechnet werden


### Basiswechsel

* die *EV* ``Î±``~``i ``~bilden eine Basis (ich glaube nicht immer, muss ich noch mal recherchieren aber vorerst tun sie das). Um Vektoren ``w`` bzgl. dieser Basis darzustellen berechnet man einfach (wie oben erklÃ¤rt) ``w``~``neu``~`` = (Î±``~``1``~`` â€¦ Î±``~``n``~``) * w``.
* Erinnerung: fÃ¼r Matrix ``A`` gilt, da/wenn sie als â€normaleâ€œ lineare Abblidung interpretiert wird:

``A``: Vek. in alter Basis â†’ Vek. in alter Basis

* Mit den *EV* ``Î±``~``i``~ von ``A`` bildet man nun zwei Matrizen, die als **Transformationsmatrizen** aufgefasst werden und eine Abbildung bzgl. der Basis bestehend aus *EV* darstellt:

``T := Mat(Î±``~``1``~, ``â€¦``, ``Î±``~``n``~``)``: Vek. in *EV*-Basis â†’ Vek. in alter Basis (von ``A``)
``T``^``-1``^`` := Mat``^``-1``^``(Î±``~``1``~, ``â€¦``, ``Î±``~``n``~``)``: Vek. in alter Basis (von ``A``) â†’ Vek. in  *EV*-Basis
(Hier war ich verwirrt, warum in beiden Definitionen die *EV* ``Î±``~``i``~ auftauchen, wÃ¤hrend ich oben noch ``e``~``i``~ und ``f``~``i``~, bzw. â€``2`` und ``1/2``â€œ verwende, und ich hatte die Vermutung einen Fehler gemacht zu haben aber dem ist nicht so. Sowohl oben als auch hier ist alles richtig. Leider ist es mir gerade zu kompliziert zu erlÃ¤utern, wie ich meine Verwirrung legen konnte. Wenn man es in Ruhe durchdenkt und die Basen richtig einsetzt, man hat nur ``e``~``i``~ und ``f``~``i``~, bzw. â€``2`` und ``1/2``â€œ zu Auswahl, kommt darauf.)
â‡’ ``A`` kann man nun mit diesen VerknÃ¼pfen (Urbilder und Basen passen Ã¼berall):
``D := T``^``-1``^`` * A * T``: Vek. in *EV*-Basis â†’ Vek. in *EV*-Basis

* ``D`` ist dabei eine **Diagonalmatrix**, wobei die *EW* auf der Diagonale (entsprechend der Reihenfolge der *EV*) stehen.
* Um mit ``D`` zu arbeiten, muss man das â€Umfeldâ€œ in dem ``A`` gegeben und â€bearbeitetâ€œ werden soll in die *EV*-Basis transformieren. Das macht man indem man jeden Vek. ``w`` mit ``T``^``-1``^ multipliziert: ``w``~``neu``~`` := T``^``-1``^`` * w``. Dann kann man die neuen Vek. ``w``~``neu``~ â€ganz normalâ€œ mit ``D`` multiplizieren âˆ’ da ``D`` eine **Diagonalmatrix** ist, klappt das auch besonders einfach und schnell ğŸ˜Šï¸.


# SingulÃ¤rwertzerlegung
Angelegt Dienstag 31 Mai 2022


* [SingulÃ¤rwertzerlegung â€“ Wikipedia](https://de.wikipedia.org/wiki/Singul%C3%A4rwertzerlegung)


# Markov-Ketten
Angelegt Donnerstag 03 Februar 2022


* KÃ¶nnen gut per Ãœbergangsmatrix beschrieben werden â‡’ Durch Potenzierung kann zukÃ¼nftige Entwicklung vorausberechnet werden.
	* [Ãœbergangsmatrix](https://de.wikipedia.org/wiki/Ãœbergangsmatrix):
		* Zeilen- oder Spaltensumme = 1, oBdA: Zeilsumme = 1, dh **(zeilen-)stochastisch**
		* alle EintrÃ¤ge zw. 0 & 1
		* Hat EW 1 (und damit eine *stationÃ¤re Verteilung*)
		* T^*^ := lim~k â†’ âˆ~ (T^k^)~i,j~ = Ï€~j~, dh. alle Elemente einer Zeile i von T^*^ sind gleich Ï€~j~, wobei Ï€ eine *stationÃ¤re Verteilung* ist.
* (Links-)EV zum EW 1 heiÃŸen [stationÃ¤re Verteilung](https://de.wikipedia.org/wiki/StationÃ¤re_Verteilung) und sind [Wahrscheinlichkeitsvektoren](https://de.wikipedia.org/wiki/Wahrscheinlichkeitsvektor), dh. (Zeilen-)Vektoren, Summe der Komponenten = 1, alle Komponenten zw. 0 & 1
	* [stationÃ¤re Verteilung](https://de.wikipedia.org/wiki/StationÃ¤re_Verteilung): Es gibt eine Startverteilung Ï€ (Zeilenvektor), der sich im Zeitverlauf nicht Ã¤ndert, dh. Ï€ * T^t^ = Ï€ fÃ¼r alle t \in N

â‡’ Ï€ ist (Links-)EV zum EW 1 (denk kurz darÃ¼ber nach, warum das so ist!).

* Eindeutig, wenn Markov-Kette **irreduzibel** (gibt ggfl. mehrere Bedingungen, lieber Wikipedia-Artikel durchlesen, um sicher zu gehen)


* Zeilenvektoren sind typisch in der Wahrscheinlichkeitstheorie


* Im Kontext von Markov-Ketten geben WS-Vektoren Ï€~0~ an, mit welcher WS man sich in dem jeweilgen Zustand befindet, dh. i-ter Eintrag von Ï€~0~ entspricht WS sich (am Anfang zum Zeitpunkt t=0) in Zustand i zu befinden.
	* Aufenthalts-WS zum Zeitpunkt t=1: Linksmultiplikation Ï€~0~ * T = Ï€~1~
	* Aufenthalts-WS zum Zeitpunkt t=k: Linksmultiplikation Ï€~0~ * T^k^ = Ï€~k~
* (Rechts-)EV zum EW 1: Absorptionswahrscheinlichkeiten im [absorbierenden Zustand](https://de.wikipedia.org/wiki/Absorbierender_Zustand) (= wird nach dem Betreten nie wieder verlassen)


Unterschiede zwischen Successor Representation und Markov-Ketten
----------------------------------------------------------------

* Beide bedienen sich des Konzepts der Ãœbergangsmatrix.
* Bei Markov-Ketten wird durch Potenzierung die Vorhersage getroffen
* Bei SR durch M = âˆ‘~t=1, ..., âˆ~ Î³^t^ * T^t^ = (E~n~ âˆ’ Î³ * T )^-1^ (geometrische Reihe), also durch eine gewichtete Summe der Ãœbergangsmatrizen


### Wichtige Wikipedia-Artikel

* <https://de.wikipedia.org/wiki/Ãœbergangskern>
* <https://de.wikipedia.org/wiki/Ãœbergangsmatrix>
* <https://de.wikipedia.org/wiki/StationÃ¤re_Verteilung>


@Ãœbergangsmatrix @Ãœbergangswahrscheinlichkeit @Markovkette @Wahrscheinlichkeit @Wahrscheinlichkeitstheorie @StationÃ¤reVerteilung @SuccessorRepresentation

# MaÃŸtheorie
Angelegt Donnerstag 17 November 2022

Ïƒ-Algebra
---------

* [id: deep-learnin-buch]Weitere Eigenschaften auf S698 in [Deep Learning Architectures: A Mathematical Approach-Springer âˆ’ Ovidiu Calin (Kommentare).pdf (Link)](./Mathematik_files/Mathematik/Deep_Learning/Deep Learning Architectures: A Mathematical Approach-Springer âˆ’ Ovidiu Calin (Kommentare).pdf)
* Ïƒ(C), CâŠ†A, beschreibt die kleinste Ïƒ-Algebra auf einer Grundemenge A, die C enthÃ¤lt und ist gegeben durch Ïƒ(C) := â‹‚~Î± ~Ïƒ~Î±~.
	* Borel-Ïƒ-Algebra: Die Ïƒ-Algebra, die von von einer Topologie erzeugt wird, dh. Ïƒ(<Alle offenen Mengen>) (Konstruktion wie oben durch Schnitte)


Messbare Funktion
-----------------

* f: (A, Ïƒ) â†’ â„ messbar, wenn f^-1^[ (a,b) ]âˆˆÏƒ, dh. wenn Urbilder messbar sind.
* *einfache Funktion*: Linearkombination aus Indikatorfunktionen:

f: (A, Ïƒ) â†’ B
f(x) := âˆ‘~i=1~^n^ a~i ~Â· 1~A_i~ (x) mit A~i~âˆˆÏƒ
![](./Mathematik_files/Mathematik/MaÃŸtheorie/pasted_image.png)

* Jede Funktion ist Grenzwert von Folge messbarer Funktionen

MaÃŸe und MaÃŸrÃ¤ume
-----------------

* Î¼: (A, Ïƒ) â†’ [0, âˆ) mit 
	* Î¼(âˆ…) = 0 (oder Î¼(A) = 1)
	* *Ïƒ-AdditivitÃ¤t*: Î¼(â‹ƒ~nâ‰¥1~ A~n~) = âˆ‘~nâ‰¥1~ Î¼(A~n~)
* (A, Ïƒ, Î¼) wird *MaÃŸraum* genannt


### Eigenschaften von MaÃŸen

* A â‹‚ B = âˆ… â‡’ Î¼(A â‹ƒ B) = Î¼(A) + Î¼(B) (*endlich additiv*)
* AâŠ†B â‡’ Î¼(A) â‰¤ Î¼(B) (*Monotonie*)
* A~n~ â†’ A â‡’ Î¼(A~n~) â†’ Î¼(A) fÃ¼r nâ†’âˆ (A~n~ â†’ A â€von innenâ€œ)
* Î¼(â‹ƒ~n~ A~n~) â‰¤ âˆ‘~n~ Î¼(A~n~)
* weitere auf S701in [#deep-learnin-buch](#Mathematik:MaÃŸtheorie)


### Verteilungsfunktion

* F(x) := Î¼((-âˆ, x)) wird [Verteilungsfunktion âˆ’ Wikipedia](https://de.wikipedia.org/wiki/Verteilungsfunktion) genannt.
	* Jeder *Verteilungsfunktion* (s. [#verteilungsfunktion-definition](#Mathematik:MaÃŸtheorie)) kann eine [WahrscheinlichkeitsmaÃŸ/Wahrscheinlichkeitsverteilung âˆ’ Wikipedia](https://de.wikipedia.org/wiki/Wahrscheinlichkeitsma%C3%9F) zugeordnet werden und umgekehrt auch.

â‡’ [Korrespondenzsatz (Stochastik) âˆ’ Wikipedia](https://de.wikipedia.org/wiki/Korrespondenzsatz_(Stochastik)).

* Mit ihm kann man praktisch folgern, dass eine *Wahrscheinlichkeitsverteilung*/*WahrscheinlichkeitsmaÃŸ* ein reellwertiges Analogon hat und mit diesem weiterarbeiten (ohne dass man ggf. weiÃŸ, wie es genau aussieht).
* Mit Hilfe des *Korrespondenzsatzes* kann man *Wahrscheinlichkeitsverteilungen*/*WahrscheinlichkeitsmaÃŸe* leichter untersuchen, da man diese in die Welt der reellwertigen Funktionen Ã¼bersetzen kann. Man kann also auf Methoden der MaÃŸtheorie verzichten und die der reellen Analysis anwenden.
* Man kann mit ihm durch Vorgabe einer komplexen *Verteilungsfunktion* wiederum komplexe *Wahrscheinlichkeitsverteilungen*/*WahrscheinlichkeitsmaÃŸe* konstruieren.


* [id: verteilungsfunktion-definition]*Verteilungsfunktionen* erfÃ¼llen im Allgemeinen folgende Eigenschaft:
	* monoton wachsend
	* rechtsstetig
	* ![](./Mathematik_files/Mathematik/MaÃŸtheorie/pasted_image001.png)

dh. Funktionen, die diese Eigenschaften haben, sind im allgemeinen *Verteilungsfunktionen*. Per F(x) := Î¼((âˆ’âˆ, x)) wird erstmal einfach eine Funktion definiert, die, wenn man es nachrechnet, die obigen Eigenschaften hat und dann die *Verteilungsfunktion von Î¼* darstellt.

Integration
-----------

* (A, Ïƒ, Î¼) MaÃŸraum, f: (A, Ïƒ, Î¼) â†’ â„ messbar
* *Integral von f bzgl. Î¼*:

Î¼(f) := âˆ«~A~ f dÎ¼ := âˆ«~A~ f(x) Î¼(dx),
bzw. fÃ¼r ein BâˆˆÏƒ:
âˆ«~B~ f dÎ¼ := âˆ«~A~ f Â· 1~A~ dÎ¼ = Î¼(f Â· 1~A~)
und im Allgemeinen gilt:
âˆ«~B~ dÎ¼ = âˆ«~A~ 1~B~ dÎ¼ = Î¼(1~A~) = Î¼(A).

* Î¼(dx) hat im Vergleich zum Riemann-Integral keine so schÃ¶ne Bedeutung mehr aber ist dennoch daran angelehnt. Man mÃ¶chte damit suggieren, dass die infinitimalen Breiten/GrundflÃ¤chen der Rechtecke/Quader nun mit Î¼ gemessen und dann mit ihrer HÃ¶he f(x) multipliziert werden.

### Wichtige SÃ¤tze zur Vertauschung von Integral und Limes
![](./Mathematik_files/Mathematik/MaÃŸtheorie/pasted_image002.png)

### MaÃŸtheoretischer Transformationssatz

* Mithilfe von Bild-MaÃŸen kann man den [:Mathematik#**Transformationssatz**](#Mathematik) auch maÃŸtheoretisch formulieren:

![](./Mathematik_files/Mathematik/MaÃŸtheorie/pasted_image004.png)
âˆ«~F~ f(h(x)) dÎ½(x) = âˆ«~F~ f(h(x)) dÎ¼âˆ˜h(x) = âˆ«~F~ f(h(x)) Â· |det( h'(x) )| dÎ¼(x),
indem man dÎ¼âˆ˜h(x) â€differenziertâ€œ, also â€innere Ableitung mal Ã¤uÃŸere Ableitungâ€œ, was h'(x) Â· dÎ¼(x) ergibt.
h'(x) packt man aus KonsistenzgrÃ¼nden in |det(Â·)|, s. dazu [:Mathematik#**Transformationssatz**](#Mathematik).


# Numerik
Angelegt Sonntag 13 November 2022


* [YouTube-Kanal mit Videos zu Finite Differenzen und Finite Elemente](https://www.youtube.com/channel/UCHZyhrpXXmbuyF8kzrsvgPg/videos)


# Optimierung
Angelegt Sonntag 13 November 2022


* Minimierung einer Vektor-wertigen Funktion ist Ã¤quivalent dazu die Komponentenfunktionen zu minimieren


# Wahrscheinlichkeitstheorie
Angelegt Dienstag 17 Mai 2022

Dichte
------
Eine Dichte ist eine Funktion, die ein MaÃŸ Î¼ im Kontext eines anderen MaÃŸes Î½ beschreibt. Sie Ã¼bersetzt sozusagen das eine MaÃŸ in das andere. Ãœber Dichten kÃ¶nnen auch MaÃŸe definiert werden. Eine Dichte wird also immer im Kontext eines MaÃŸes Î¼ bzgl des zweiten MaÃŸes Î½ angegeben (wobei Î¼ aber noch Ã¶fter Î½ unterdrÃ¼ckt werden).
Bsp: Die KÃ¶rpergrÃ¶ÃŸe folgt der Normalverteilung. Ein MaÃŸ, bzw. MaÃŸraum, der das beschreibt wÃ¤re relativ abstrakt. Um dies zu beheben gibt es die Glockkurve, die die Dichte der Normalverteilung bzgl. des Lebesgue-MaÃŸes darstellt. Mit dieser Funktion kann man besser arbeiten, da sie auf â„ definiert ist

# Weitz
Angelegt Samstag 26 MÃ¤rz 2022

[Was sind Distributionen?](https://www.youtube.com/watch?v=J8Gfq11eBlY) â† Dieses Video ist sehr interessant!

* Integralnotation bei Testfunktionen ist nur Notationsmissbrauch! Wird in Video genauer erklÃ¤rt! Die Dirac-Distribution liefert einfach den Funktionswert an der Stelle 0, Integral hin- oder her.
* Wenn man Funktionen verallgemeinert, mÃ¶chte man so viele Eigenschaften wie mÃ¶glich retten. Im Zuge dessen die Eigenschaft Zahlen auf Zahlen abzubilden, scheitert man aber.
	* Beispiel: Raum der quadratintegrierbaren Funktionen LÂ². Dessen Elemente sind Ã„quivalenzklassen von Funktionen, und eine Ã„quivalenzklasse kann keine Zahlen abbilden. Hier ist diese Eigenschaft also schon verlorengegangen (Beim Integrieren behandelt man sie aber weiterhin wie Funktionen).


[Integrale sind kontinuierliche Summen](https://www.youtube.com/watch?v=_GTbrbv4-qk)
[Lebesgue-Integral versus Riemann-Integral](https://www.youtube.com/watch?v=uUkDdz48myo)
[Integration durch Substitution](https://www.youtube.com/watch?v=geJ-36mnZ1I)

Warum Fixpunkte interessant sind
--------------------------------
Gleichungssysteme (vor allem mit {gewÃ¶hnlichen, partiellen} Differentialgleichungen sind oft mit dem Finden von Nullstellen zu lÃ¶sen.
![](./Mathematik_files/Mathematik/Weitz/pasted_image.png)
Wie man oben sieht, kann man durch Addition von x eine neue Funktion G(x) definieren und kann, wenn man Ã¼ber die Existenz von Fixpunkten bescheid weiÃŸ, auch Aussagen Ã¼ber die Nullstellen treffen.
Das Finden einer NST von F ist damit Ã¤quivalent zum Finden eines Fixpunktes von G.

